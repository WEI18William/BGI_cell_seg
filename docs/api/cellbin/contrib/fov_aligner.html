<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>cellbin.contrib.fov_aligner API documentation</title>
<meta name="description" content="Matcher provide the way to get overlap/offset from neighbor FOV." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cellbin.contrib.fov_aligner</code></h1>
</header>
<section id="section-intro">
<p>Matcher provide the way to get overlap/offset from neighbor FOV.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Matcher provide the way to get overlap/offset from neighbor FOV.
&#39;&#39;&#39;

import numpy as np
import cv2 as cv
import tifffile
import itertools
from tqdm import tqdm
from multiprocessing import Process, Manager, Pool, cpu_count
from cellbin.utils.file_manager import rc_key
import copy
import time

from cellbin.utils import clog
from cellbin.image import Image

class FOVAligner(object):
    &#39;&#39;&#39;
    计算出所有图像的偏移量
    &#39;&#39;&#39;
    def __init__(self, images_path, rows, cols, mode=&#39;FFT&#39;, multi=True, channel=0):
        self.set_size(rows, cols)
        self.set_images_path(images_path)
        self.set_channel(channel)
        self.multi = multi # TODO 多进程选项
        self.matcher = FFTMatcher() if mode == &#39;FFT&#39; else SIFTMatcher()
        self.num = 5
        self.__init_value = 999

        self.horizontal_jitter = np.zeros((self.rows, self.cols, 2), dtype=int) + self.__init_value
        self.vertical_jitter = np.zeros((self.rows, self.cols, 2), dtype=int) + self.__init_value
        self.fov_mask = np.zeros((self.rows, self.cols, 3))

    def set_channel(self, channel):
        assert isinstance(channel, int), &#34;Channel type error.&#34;
        self.channel = channel

    def set_images_path(self, images_path):
        # assert isinstance(images_path, dict), &#34;Image path type error.&#34;
        self.images_path = images_path

    def set_size(self, rows, cols):
        assert isinstance(rows, int) or isinstance(rows, float), &#34;Rows type error.&#34;
        assert isinstance(cols, int) or isinstance(cols, float), &#34;Cols type error.&#34;
        self.rows = rows
        self.cols = cols

    def set_process(self, p):
        self.num = p
        if self.num &lt;= 1:
            self.multi = False  # @lizepeng add on 2023/05/17

    def create_jitter(self):
        &#39;&#39;&#39;
        Scan image and create offset.
        &#39;&#39;&#39;
        confi_mask = np.zeros((self.rows, self.cols, 2)) - 1
        ncc_confi_mask = np.zeros((self.rows, self.cols, 2)) - 1

        if self.multi:
            tesk_list = list()
            for row_index in range(self.rows):
                tesk_list.append({&#39;row&#39;: row_index})
            for col_index in range(self.cols):
                tesk_list.append({&#39;col&#39;: col_index})

            with Manager() as manager:
                h_j = manager.dict()
                v_j = manager.dict()
                confi_h = manager.dict()
                confi_v = manager.dict()
                # process_list = []
                # num = int(cpu_count() / 2)
                # num = 10
                clog.info(f&#34;stitch using {self.num} process&#34;)
                pool = Pool(processes=self.num)

                for tesk in tesk_list:
                    if list(tesk.keys())[0] == &#39;row&#39;:
                        row_index = tesk[&#39;row&#39;]
                        pool.apply_async(func=self._multi_jitter, args=(h_j, confi_h, row_index, None, 0))

                    elif list(tesk.keys())[0] == &#39;col&#39;:
                        col_index = tesk[&#39;col&#39;]
                        pool.apply_async(func=self._multi_jitter, args=(v_j, confi_v, None, col_index, 1))

                    else:
                        pass

                pool.close()
                pool.join()

                for key in h_j.keys():
                    row, col = [int(i) for i in key.split(&#39;_&#39;)]
                    self.horizontal_jitter[row, col, :] = h_j[key]
                    confi_mask[row, col, 0] = confi_h[key]

                for key in v_j.keys():
                    row, col = [int(i) for i in key.split(&#39;_&#39;)]
                    self.vertical_jitter[row, col, :] = v_j[key]
                    confi_mask[row, col, 1] = confi_v[key]
        else:
            clog.info(&#34;Scan Row by Row.&#34;)
            for i in tqdm(range(self.rows), desc=&#39;RowByRow&#39;):
                train = self._get_image(i, 0)
                for j in range(1, self.cols):
                    query = self._get_image(i, j)
                    if (train is not None) and (query is not None):
                        if np.max(train) == 0 or np.max(query) == 0: b = None
                        elif train.shape == query.shape:
                            try: b = self.matcher.neighbor_match(train, query, 0)
                            except: b = None
                        else: b = None
                        if b is not None:
                            self.horizontal_jitter[i, j, :] = b[:2]
                            confi_mask[i, j, 0] = b[2]
                            ncc_confi_mask[i, j, 0] = b[3]
                    train = query

            clog.info(&#34;Scan Col by Col.&#34;)
            for j in tqdm(range(self.cols), desc=&#39;ColByCol&#39;):
                train = self._get_image(0, j)
                for i in range(1, self.rows):
                    query = self._get_image(i, j)
                    if (train is not None) and (query is not None):
                        if np.max(train) == 0 or np.max(query) == 0: b = None
                        elif train.shape == query.shape:
                            try: b = self.matcher.neighbor_match(train, query, 1)
                            except: b = None
                        else: b = None
                        if b is not None:
                            self.vertical_jitter[i, j, :] = b[:2]
                            confi_mask[i, j, 1] = b[2]
                            ncc_confi_mask[i, j, 1] = b[3]
                    train = query

        self.horizontal_jitter, confi_mask[:, :, 0], _ = self.filter_abnormal_offset(self.horizontal_jitter,
                                                                                     confi_mask[:, :, 0],
                                                                                     thread=5)

        self.vertical_jitter, confi_mask[:, :, 1], _ = self.filter_abnormal_offset(self.vertical_jitter,
                                                                                   confi_mask[:, :, 1],
                                                                                   thread=5)

        self.fov_mask[:, :, :2] = confi_mask
        self.fov_mask[:, :, 2] = np.max(confi_mask, axis=2)
        self.fov_mask[:, :, 2][np.where(self.fov_mask[:, :, 2] &gt;= 99)] = 99
        clog.info(&#34;Scan image done!&#34;)

    def _offset_eval(self, height, width, overlap=0.1):
        &#34;&#34;&#34;
        :param height: fov image height
        :param width: fov image width
        :param overlap:
        :return: offset matrix r * c * 1
        &#34;&#34;&#34;
        h_j = copy.deepcopy(self.horizontal_jitter)
        v_j = copy.deepcopy(self.vertical_jitter)

        for r in range(self.rows):
            for c in range(self.cols):
                if h_j[r, c, 0] != -self.__init_value and \
                        h_j[r, c, 0] != self.__init_value:
                    h_j[r, c, 0] += int(width * overlap)
                if v_j[r, c, 1] != -self.__init_value and \
                        v_j[r, c, 1] != self.__init_value:
                    v_j[r, c, 1] += int(height * overlap)

        h_e = (h_j[:, :, 0] ** 2 + h_j[:, :, 1] ** 2) ** 0.5
        h_e[np.where(h_e &gt;= self.__init_value)] = -1

        v_e = (v_j[:, :, 0] ** 2 + v_j[:, :, 1] ** 2) ** 0.5
        v_e[np.where(v_e &gt;= self.__init_value)] = -1

        e = np.max(np.stack((h_e.astype(int), v_e.astype(int)), axis=2), axis=2)

        return e, h_e, v_e

    def _multi_jitter(self, jitter, confi, row=None, col=None, axis=0):
        if axis == 0:
            train = self._get_image(row, 0)
            for col_index in range(1, self.cols):
                query = self._get_image(row, col_index)
                if (train is not None) and (query is not None):
                    if np.max(train) == 0 or np.max(query) == 0: b = None
                    elif train.shape == query.shape:
                        try: b = self.matcher.neighbor_match(train, query, 0)
                        except: b = None
                    else: b = None
                    jitter[rc_key(row, col_index)] = b[:2]
                    confi[rc_key(row, col_index)] = b[2]
                train = query

        elif axis == 1:
            train = self._get_image(0, col)
            for row_index in range(1, self.rows):
                query = self._get_image(row_index, col)
                if (train is not None) and (query is not None):
                    if np.max(train) == 0 or np.max(query) == 0: b = None
                    elif train.shape == query.shape:
                        try: b = self.matcher.neighbor_match(train, query, 1)
                        except: b = None
                    else: b = None
                    jitter[rc_key(row_index, col)] = b[:2]
                    confi[rc_key(row_index, col)] = b[2]
                train = query
        else:
            clog.info(&#34;axis value error.&#34;)

    def _get_image(self, row, col):
        &#34;&#34;&#34;
        row: fov in row
        col: fov in col
        &#34;&#34;&#34;
        temp_index = list(self.images_path.keys())[0]
        if len(temp_index.split(&#39;_&#39;)[0]) == 4:
            key = rc_key(row, col)
        else: key = rc_key(row, col, source=True)
        if type(self.images_path) is dict:
            if key not in self.images_path.keys():
                return None
            img = Image()
            img.read(self.images_path[key])
            arr = img.image
            if arr.ndim == 3:
                arr = arr[:, :, self.channel]
            return arr

    @staticmethod
    def filter_abnormal_offset(offset: np.array, stitch_confi_mask: np.ndarray, thread: int = 2):
        &#34;&#34;&#34;
        过滤异常的offset
        :param offset: W*H*C
        :return:
        &#34;&#34;&#34;
        c = offset.shape[2]
        max_thread = 0
        for i in range(c):
            offset_1 = offset[:, :, i]  # 所有offset
            valid_offset = offset_1[np.where((offset_1 != -999) &amp; (stitch_confi_mask &gt; thread))]
            # 去掉最大值和最小值 取出[0.2, 0.8]之间的值来求均值和方差
            if len(valid_offset) &gt; 3:
                percentile = np.percentile(valid_offset, (25, 50, 75), interpolation=&#39;midpoint&#39;)
                Q1, Q3 = percentile[0], percentile[2]
                IQR = Q3 - Q1
                IQR = IQR if IQR &gt; 5 else IQR + 3

                thread_max = Q3 + 1.5 * IQR
                thread_min = Q1 - 1.5 * IQR

                error_fov = np.zeros(shape=(0, 2), dtype=np.int32)
                max_error_fov = np.array(np.where((offset_1 &gt; thread_max))).T
                min_error_fov = np.array(np.where((offset_1 &lt; thread_min) &amp; (offset_1 != -999))).T
                error_fov = np.vstack([error_fov, max_error_fov, min_error_fov])
                if len(error_fov) &gt; 0:
                    for row, col in error_fov:
                        if stitch_confi_mask[row, col] &gt; thread * 3:  # 置信度高, 阈值放宽
                            thread_max += IQR * 0.6
                            thread_min -= IQR * 0.6
                        if offset_1[row, col] &gt; thread_max or offset_1[row, col] &lt; thread_min:
                            offset[row, col] = -999
                            stitch_confi_mask[row, col] = 0

                if thread_max &gt; max_thread:
                    max_thread = thread_max
        return offset, stitch_confi_mask, max_thread


class Matcher(object):

    def __init__(self):
        self.overlap = 0.1
        self.horizontal = True

    def get_roi(self, arr0, arr1):
        &#34;&#34;&#34;
        get overlap area
        :param arr0: fov 1
        :param arr1: fov 2
        :return: overlap area
        &#34;&#34;&#34;
        h0, w0 = arr0.shape
        h1, w1 = arr1.shape
        w = int(min(w0, w1) * self.overlap)
        h = int(min(h0, h1) * self.overlap)

        if self.horizontal:
            x0, y0, x1, y1 = [w0 - w, 0, w0, h0]
            x0_, y0_, x1_, y1_ = [0, 0, w, h1]
        else:
            x0, y0, x1, y1 = [0, h0 - h, w0, h0]
            x0_, y0_, x1_, y1_ = [0, 0, w1, h]
        return arr0[y0: y1, x0: x1], arr1[y0_: y1_, x0_: x1_]

    @staticmethod
    def slice_image(image: np.ndarray, slice_width: int, slice_height: int,
                    overlap_height_ratio=0.2, overlap_width_ratio=0.2):
        &#34;&#34;&#34;
        Image is cropped and stacked
        &#34;&#34;&#34;
        if image.ndim == 3:
            image_height, image_width, _ = image.shape
        else:
            image_height, image_width = image.shape

        y_overlap = int(overlap_height_ratio * slice_height)
        x_overlap = int(overlap_width_ratio * slice_width)

        slice_bboxes = []
        y_max = y_min = 0
        while y_max &lt; image_height:
            x_min = x_max = 0
            y_max = y_min + slice_height
            while x_max &lt; image_width:
                x_max = x_min + slice_width
                if y_max &gt; image_height or x_max &gt; image_width:
                    xmax = min(image_width, x_max)
                    ymax = min(image_height, y_max)
                    xmin = max(0, xmax - slice_width)
                    ymin = max(0, ymax - slice_height)
                    slice_bboxes.append([xmin, ymin, xmax, ymax])
                else:
                    slice_bboxes.append([x_min, y_min, x_max, y_max])
                x_min = x_max - x_overlap
            y_min = y_max - y_overlap

        n_ims = 0
        sliced_image_result = []
        assert len(slice_bboxes) &gt; 0, &#34;slice image must &gt; 0&#34;
        for slice_bbox in slice_bboxes:
            n_ims += 1
            # extract image
            tlx = slice_bbox[0]
            tly = slice_bbox[1]
            brx = slice_bbox[2]
            bry = slice_bbox[3]
            image_pil_slice = image[tly:bry, tlx:brx]
            sliced_image_result.append(image_pil_slice)
        return sliced_image_result


class SIFTMatcher(Matcher):
    def __init__(self, ):
        super(SIFTMatcher, self).__init__()
        self.sift = cv.SIFT_create()
        self.__matcher = self.matcher()

    def neighbor_match(self, train, query, axis=0):
        if axis == 0:
            self.horizontal = True
        else: self.horizontal = False

        train_local, query_local = self.get_roi(train, query)
        train_local = np.array(train_local, dtype=np.uint8)
        query_local = np.array(query_local, dtype=np.uint8)
        return self.sift_match(train_local, query_local)

    def sift_match(self, train, query):
        psd_kp1, psd_des1 = self.sift.detectAndCompute(train, None)
        kps1 = np.float32([kp.pt for kp in psd_kp1])
        psd_kp2, psd_des2 = self.sift.detectAndCompute(query, None)
        kps2 = np.float32([kp.pt for kp in psd_kp2])
        if len(kps1) == 0 or len(kps2) == 0:
            return None
        try:
            matches = self.__matcher.knnMatch(psd_des1, psd_des2, k=2)
        except:
            return None
        local_match = list()

        for m, n in matches:
            if m.distance &lt; 0.5 * n.distance: local_match.append((m.trainIdx, n.queryIdx))

        if len(local_match) == 0:
            return None
        else:
            pts_a = np.float32([kps1[i] for (_, i) in local_match])
            pts_b = np.float32([kps2[i] for (i, _) in local_match])
            if self.horizontal:
                pts_b[:, 0] += train.shape[1]
            else:
                pts_b[:, 1] += train.shape[0]
            offset = np.median(pts_a - pts_b, axis=0)
            return [offset[0], offset[1], 100]

    @staticmethod
    def matcher():
        flann_index_kd_tree = 1
        index_params = dict(algorithm=flann_index_kd_tree, trees=5)
        search_params = dict(checks=50)
        flann = cv.FlannBasedMatcher(index_params, search_params)
        return flann


class FFTMatcher(Matcher):
    def __init__(self):
        super(FFTMatcher, self).__init__()

    def pcm_peak(self, image1: np.ndarray, image2: np.ndarray):
        &#34;&#34;&#34;
        Compute peak correlation matrix for two images and Interpret the translation to find the translation with heighest ncc
        :param image1:the first image (the dimension must be 2)
        :param image2:the second image (the dimension must be 2)
        :return:
        _ncc : the highest ncc
        x : the selected x position
        y : the selected y position
        sub_image1: the overlapping area of image1
        sub_image2: the overlapping area of image2
        &#34;&#34;&#34;
        assert image1.ndim == 2, &#34;the dimension must be 2&#34;
        assert image2.ndim == 2, &#34;the dimension must be 2&#34;
        size_y, size_x = image1.shape[:2]
        pcm = self._pcm(image1, image2).real
        yins, xins, pcm_ = self._multi_peak_max(pcm)
        lims = np.array([[-size_y, size_y], [-size_x, size_x]])
        max_peak = self._interpret_translation(
            image1, image2, yins, xins, *lims[0], *lims[1]
        )  # 与输入进来的位置参数刚好相反
        ncc, offset_y, offset_x, sub_dst, sub_src = max_peak
        return ncc, offset_y, offset_x, sub_dst, sub_src

    def neighbor_match(self, src_img, dst_img, axis=0):
        &#34;&#34;&#34;
        Calculate the relative stitch offsets of adjacent FOVs
        :param src_img: [NxM], image of source, A is number of image to calculate cross-power specturm
        :param dst_img:[NxM], image of destination
        :return: offset, confidence
        &#34;&#34;&#34;
        if axis == 0:
            self.horizontal = True
        else: self.horizontal = False

        src_img, dst_img = self.get_roi(src_img, dst_img)
        ncc, offset_y, offset_x, sub_dst, sub_src = self.pcm_peak(src_img, dst_img)
        # Local image confirmation
        if sub_dst is not None:
            sub_dst_crop, sub_src_crop = self._slice_images(sub_dst, sub_src)
        else:
            return None
        if len(sub_src_crop) &gt; 0:
            (y0, x0), confidence0, fft_image = self._stack_peak_pc(sub_dst_crop, sub_src_crop)
            offset_x += x0
            offset_y += y0
        else:
            confidence0 = ncc

        size_y, size_x = dst_img.shape[:2]
        if self.horizontal:
            offset_x = offset_x - size_x
        else:
            offset_y = offset_y - size_y

        return [offset_x, offset_y, confidence0 * 100, ncc * 100]

    @staticmethod
    def ncc(image1, image2):
        &#34;&#34;&#34;Compute the normalized cross correlation for two images.
        Parameters
        ---------
        image1 : the first image (the dimension must be 2)
        image2 : the second image (the dimension must be 2)
        Returns
        -------
        ncc : the normalized cross correlation
        &#34;&#34;&#34;
        assert image1.ndim == 2
        assert image2.ndim == 2
        assert np.array_equal(image1.shape, image2.shape)
        image1 = image1.flatten()
        image2 = image2.flatten()
        n = np.dot(image1 - np.mean(image1), image2 - np.mean(image2))
        d = np.linalg.norm(image1) * np.linalg.norm(image2)
        if d &lt; 0.0001:
            return 0
        return n / d

    @staticmethod
    def extract_overlap_subregion(image, y: int, x: int):
        &#34;&#34;&#34;Extract the overlapping subregion of the image.
        Parameters
        ---------
        image : the image (the dimension must be 2)
        y : the y (second last dim.) position
        x : the x (last dim.) position
        Returns
        subimage : the extracted subimage
        &#34;&#34;&#34;
        sizeY = image.shape[0]
        sizeX = image.shape[1]
        assert (np.abs(y) &lt; sizeY) and (np.abs(x) &lt; sizeX)
        # clip x to (0, size_Y)
        h_start = int(max(0, min(y, sizeY, key=int), key=int))
        # clip x+sizeY to (0, size_Y)
        h_end = int(max(0, min(y + sizeY, sizeY, key=int), key=int))
        w_start = int(max(0, min(x, sizeX, key=int), key=int))
        w_end = int(max(0, min(x + sizeX, sizeX, key=int), key=int))
        return image[h_start:h_end, w_start:w_end]

    def _slice_images(self, src_img, dst_img):
        &#34;&#34;&#34;
        crop and stack images
        :param src_img: template image
        :param dst_img: background image
        :return: flod images
        &#34;&#34;&#34;
        ratio = 0.7
        overlap_size = min(dst_img.shape[:2])
        width = overlap_size if self.horizontal else int(overlap_size * ratio)
        height = int(overlap_size * ratio) if self.horizontal else overlap_size
        src_slice_img = self.slice_image(src_img, slice_width=width, slice_height=height,
                                         overlap_height_ratio=0.2, overlap_width_ratio=0.2, )
        dst_slice_img = self.slice_image(dst_img, slice_width=width, slice_height=height,
                                         overlap_height_ratio=0.2, overlap_width_ratio=0.2, )
        src_slice_img, dst_slice_img = self._fliter_null_information_image(src_slice_img, dst_slice_img)
        return np.array(src_slice_img), np.array(dst_slice_img)

    def _stack_peak_pc(self, stack_im0, stack_im1):
        &#34;&#34;&#34;
        Computes phase correlation between stack im0 and stack im1 and Interpret the translation to find the translation
        Args:
            stack_im0: the first stack image, N*W*H
            stack_im1: the second stack image, N*W*H
        Returns:
            ret: location
            success: matcher degree
            scps_result: phase correlation image
        &#34;&#34;&#34;
        # TODO: Implement some form of high-pass filtering of PHASE correlation
        stack_im0 = [self._apodize(im) for im in stack_im0]
        stack_im1 = [self._apodize(im) for im in stack_im1]
        f0, f1 = [np.fft.fft2(arr) for arr in (stack_im0, stack_im1)]
        # spectrum can be filtered (already),
        # so we have to take precaution against dividing by 0
        eps = abs(f1).max() * 1e-15
        cps = abs(np.fft.ifft2((f0 * f1.conjugate()) / (abs(f0) * abs(f1) + eps)))
        # scps = shifted cps
        scps = np.fft.fftshift(cps)
        scps_result = np.zeros_like(scps[0])
        sum_weights = 0 + 0.00000001
        for i, scp in enumerate(scps):
            scp = cv.GaussianBlur(scp, (7, 7), 1)
            thread = np.percentile(scp, 95)
            scp[np.where(scp &lt; thread)] = 0
            (_, _), success = self._argmax_translation(scp)
            scps_result += scp * success
            sum_weights += success
        scps_result = scps_result / sum_weights
        (t0, t1), success = self._argmax_translation(scps_result)  # (y,x)
        if success &lt; 0.05:
            scps_result = cv.GaussianBlur(scps_result, (7, 7), 1)
            win = cv.createHanningWindow((7, 7), cv.CV_32F)
            scps_result = cv.filter2D(scps_result, -1, win)
            (t0, t1), _ = self._argmax_translation(scps_result)  # (y,x)

        ret = np.array((np.round(t0), np.round(t1)))

        # _compensate_fftshift is not appropriate here, this is OK.
        t0 -= scps[0].shape[0] // 2
        t1 -= scps[0].shape[1] // 2

        ret -= np.array(scps[0].shape, int) // 2
        return ret, success, scps_result

    def _interpret_translation(self,
                               image1: np.ndarray,
                               image2: np.ndarray,
                               yins: np.ndarray,
                               xins: np.ndarray,
                               ymin: int,
                               ymax: int,
                               xmin: int,
                               xmax: int,
                               n: int = 2,
                               ):
        &#34;&#34;&#34;Interpret the translation to find the translation with heighest ncc.
        Parameters
        ---------
        image1 : the first image (the dimension must be 2)
        image2 : the second image (the dimension must be 2)
        yins : the y positions estimated by PCM
        xins : the x positions estimated by PCM
        ymin : the minimum value of y (second last dim.)
        ymax : the maximum value of y (second last dim.)
        xmin : the minimum value of x (last dim.)
        xmax : the maximum value of x (last dim.)
        n : the number of peaks to check, default is 2.
        Returns
        _ncc : the highest ncc
        x : the selected x position
        y : the selected y position
        &#34;&#34;&#34;
        assert image1.ndim == 2
        assert image2.ndim == 2
        assert np.array_equal(image1.shape, image2.shape)
        sizeY = image1.shape[0]
        sizeX = image1.shape[1]
        assert np.all(0 &lt;= yins) and np.all(yins &lt; sizeY)
        assert np.all(0 &lt;= xins) and np.all(xins &lt; sizeX)

        _ncc = -np.infty
        x, y = 0, 0
        overlap_img_1 = None
        overlap_img_2 = None

        ymagss = [yins, sizeY - yins]
        ymagss[1][ymagss[0] == 0] = 0
        xmagss = [xins, sizeX - xins]
        xmagss[1][xmagss[0] == 0] = 0

        # concatenate all the candidates
        _poss = []
        for ymags, xmags, ysign, xsign in itertools.product(
                ymagss, xmagss, [-1, +1], [-1, +1]
        ):
            yvals = ymags * ysign
            xvals = xmags * xsign
            _poss.append([yvals, xvals])
        poss = np.array(_poss)
        valid_ind = (
                (ymin &lt;= poss[:, 0, :])
                &amp; (poss[:, 0, :] &lt;= ymax)
                &amp; (xmin &lt;= poss[:, 1, :])
                &amp; (poss[:, 1, :] &lt;= xmax)
        )
        assert np.any(valid_ind)
        valid_ind = np.any(valid_ind, axis=0)
        for pos in np.moveaxis(poss[:, :, valid_ind], -1, 0)[: int(n)]:
            for yval, xval in pos:
                if (ymin &lt;= yval) and (yval &lt;= ymax) and (xmin &lt;= xval) and (xval &lt;= xmax):
                    subI1 = self.extract_overlap_subregion(image1, yval, xval)
                    subI2 = self.extract_overlap_subregion(image2, -yval, -xval)
                    if subI1.size / (sizeX * sizeY) &gt; 0.05 and min(subI1.shape) &gt; 80:  # 在overlap区域, 最少10%的重叠区域
                        ncc_val = self.ncc(subI1, subI2)
                        if ncc_val &gt; _ncc:
                            _ncc = float(ncc_val)
                            y = int(yval)
                            x = int(xval)
                            overlap_img_1 = subI1
                            overlap_img_2 = subI2
        return _ncc, y, x, overlap_img_1, overlap_img_2

    def _get_success(self, array, coord, radius=2):
        &#34;&#34;&#34;
        Given a coord, examine the array around it and return a number signifying
        how good is the &#34;match&#34;.

        Args:
            radius: Get the success as a sum of neighbor of coord of this radius
            coord: Coordinates of the maximum. Float numbers are allowed
                (and converted to int inside)

        Returns:
            Success as float between 0 and 1 (can get slightly higher than 1).
            The meaning of the number is loose, but the higher the better.
        &#34;&#34;&#34;
        coord = np.round(coord).astype(int)
        coord = tuple(coord)

        subarr = self._get_subarr(array, coord, radius)

        theval = subarr.sum()
        theval2 = array[coord]
        # bigval = np.percentile(array, 97)
        # success = theval / bigval
        # TODO: Think this out
        success = np.sqrt(theval * theval2)
        return success

    def _argmax_translation(self, array):
        # We want to keep the original and here is obvious that
        # it won&#39;t get changed inadvertently
        array_orig = array.copy()
        ashape = np.array(array.shape, int)
        mask = np.ones(ashape, float)
        array *= mask

        # WE ARE FFTSHIFTED already.
        # ban translations that are too big
        aporad = (ashape // 6).min()
        mask2 = self._get_apofield(ashape, aporad, corner=True)
        array *= mask2
        # Find what we look for
        tvec = self._argmax_ext(array, &#39;inf&#39;)
        tvec = self._interpolate(array_orig, tvec)

        # If we use constraints or min filter,
        # array_orig[tvec] may not be the maximum
        success = self._get_success(array_orig, tuple(tvec), 2)

        return tvec, success

    def _interpolate(self, array, rough, rad=2):
        &#34;&#34;&#34;
        Returns index that is in the array after being rounded.

        The result index tuple is in each of its components between zero and the
        array&#39;s shape.
        &#34;&#34;&#34;
        rough = np.round(rough).astype(int)
        surroundings = self._get_subarr(array, rough, rad)
        com = self._argmax_ext(surroundings, 1)
        offset = com - rad
        ret = rough + offset
        # similar to win.wrap, so
        # -0.2 becomes 0.3 and then again -0.2, which is rounded to 0
        # -0.8 becomes - 0.3 -&gt; len() - 0.3 and then len() - 0.8,
        # which is rounded to len() - 1. Yeah!
        ret += 0.5
        ret %= np.array(array.shape).astype(int)
        ret -= 0.5
        return ret

    def _apodize(self, what, aporad=None):
        &#34;&#34;&#34;
        Given an image, it apodizes it (so it becomes quasi-seamless).
        When ``ratio`` is None, color near the edges will converge
        to the same colour, whereas when ratio is a float number, a blurred
        original image will serve as background.

        Args:
            what: The original image
            aporad (int): Radius [px], width of the band near the edges
                that will get modified
        Returns:
            The apodized image
        &#34;&#34;&#34;
        if aporad is None:
            mindim = min(what.shape)
            aporad = int(mindim * 0.12)
        apofield = self._get_apofield(what.shape, aporad)
        res = what * apofield
        bg = self._get_borderval(what, aporad // 2)
        res += bg * (1 - apofield)
        return res

    @staticmethod
    def _get_borderval(img, radius=None):
        &#34;&#34;&#34;
        Given an image and a radius, examine the average value of the image
        at most radius pixels from the edge
        &#34;&#34;&#34;
        if radius is None:
            mindim = min(img.shape)
            radius = max(1, mindim // 20)
        mask = np.zeros_like(img, dtype=bool)
        mask[:, :radius] = True
        mask[:, -radius:] = True
        mask[radius, :] = True
        mask[-radius:, :] = True

        mean = np.median(img[mask])
        return mean

    @staticmethod
    def _get_apofield(shape, aporad, corner=False):
        &#34;&#34;&#34;
        Returns an array between 0 and 1 that goes to zero close to the edges.
        &#34;&#34;&#34;
        if aporad == 0:
            return np.ones(shape, dtype=float)
        apos = np.hanning(aporad * 2)
        vecs = []
        for dim in shape:
            assert dim &gt; aporad * 2, \
                &#34;Apodization radius %d too big for shape dim. %d&#34; % (aporad, dim)
            toapp = np.ones(dim)
            toapp[:aporad] = apos[:aporad]
            toapp[-aporad:] = apos[-aporad:]
            vecs.append(toapp)
        apofield = np.outer(vecs[0], vecs[1])
        if corner:
            apofield[aporad:-aporad] = 1
            apofield[:, aporad:-aporad] = 1

        return apofield

    @staticmethod
    def _multi_peak_max(PCM):
        &#34;&#34;&#34;Find the first to n th largest peaks in PCM.
        PCM : the peak correlation matrix
        Returns
        rows : the row indices for the peaks
        cols : the column indices for the peaks
        vals : the values of the peaks
        &#34;&#34;&#34;
        row, col = np.unravel_index(np.argsort(PCM.ravel()), PCM.shape)
        vals = PCM[row[::-1], col[::-1]]
        return row[::-1], col[::-1], vals

    @staticmethod
    def _pcm(image1, image2):
        &#34;&#34;&#34;Compute peak correlation matrix for two images.
        image1 :  the first image (the dimension must be 2)
        image2 : the second image (the dimension must be 2)
        Returns
        PCM : the peak correlation matrix
        &#34;&#34;&#34;
        assert image1.ndim == 2
        assert image2.ndim == 2
        assert np.array_equal(image1.shape, image2.shape)
        F1 = np.fft.fft2(image1)
        F2 = np.fft.fft2(image2)
        FC = F1 * np.conjugate(F2)
        return np.fft.ifft2(FC / (np.abs(FC) + 0.0000000001)).real.astype(np.float32)

    @staticmethod
    def _fliter_null_information_image(src_imgs, dst_imgs):
        &#34;&#34;&#34;
        Filter out sub img that has no information
        :param src_imgs: first crop and stack images
        :param dst_imgs: second crop and stack images
        :return: valid sub images
        &#34;&#34;&#34;
        src_lists = []
        dst_lists = []
        fov_deepth = 8 if src_imgs[0].dtype == np.uint8 else 16
        img_grad_thread = 0.08 * (2 ** fov_deepth / 256) + 0.12
        for src, dst in zip(src_imgs, dst_imgs):
            src_grad = cv.Sobel(src, -1, 1, 1).std()
            dst_grad = cv.Sobel(dst, -1, 1, 1).std()
            if max(src_grad, dst_grad) &gt; img_grad_thread:
                src_lists.append(src)
                dst_lists.append(dst)
        return src_lists, dst_lists

    @staticmethod
    def _argmax_ext(array, exponent):
        &#34;&#34;&#34;
        Calculate coordinates of the COM (center of mass) of the provided array.
        Args:
            array (ndarray): The array to be examined.
            exponent (float or &#39;inf&#39;): The exponent we power the array with. If the
                value &#39;inf&#39; is given, the coordinage of the array maximum is taken.
        Returns:
            np.ndarray: The COM coordinate tuple, float values are allowed!
        &#34;&#34;&#34;

        # When using an integer exponent for _argmax_ext, it is good to have the
        # neutral rotation/scale in the center rather near the edges
        if exponent == &#34;inf&#34;:
            amax = np.argmax(array)
            ret = list(np.unravel_index(amax, array.shape))
        else:
            col = np.arange(array.shape[0])[:, np.newaxis]
            row = np.arange(array.shape[1])[np.newaxis, :]

            arr2 = array ** exponent
            arrsum = arr2.sum()
            if arrsum == 0:
                # We have to return SOMETHING, so let&#39;s go for (0, 0)
                return np.zeros(2)
            arrprody = np.sum(arr2 * col) / arrsum
            arrprodx = np.sum(arr2 * row) / arrsum
            ret = [arrprody, arrprodx]
            # We don&#39;t use it, but it still tells us about value distribution

        return np.array(ret)

    @staticmethod
    def _get_subarr(array, center, rad):
        &#34;&#34;&#34;
        Args:
            array (ndarray): The array to search
            center (2-tuple): The point in the array to search around
            rad (int): Search radius, no radius (i.e. get the single point)
                implies rad == 0
        &#34;&#34;&#34;
        dim = 1 + 2 * rad
        subarr = np.zeros((dim,) * 2)
        corner = np.array(center) - rad
        for ii in range(dim):
            yidx = corner[0] + ii
            yidx %= array.shape[0]
            for jj in range(dim):
                xidx = corner[1] + jj
                xidx %= array.shape[1]
                subarr[ii, jj] = array[yidx, xidx]
        return subarr

    # @staticmethod
    # def _show_3d(image):
    #     from matplotlib import cm
    #     import matplotlib.pyplot as plt
    #     h, w = image.shape
    #     X = np.arange(0, w, 1)
    #     Y = np.arange(0, h, 1)
    #     X, Y = np.meshgrid(X, Y)
    #     fig = plt.figure()
    #     ax = fig.add_subplot(projection=&#39;3d&#39;)
    #     ax.plot_surface(X, Y, image, cmap=cm.coolwarm)
    #     plt.show()


if __name__ == &#39;__main__&#39;:
    import matplotlib.pyplot as plt

    dst_path = r&#34;D:\05_kuisu\project\06_stitchQC\Y00038K5\images\Y00038K5\Y00038K5_0007_0007_2022-12-30_16-10-08-614.tif&#34;
    src_path = r&#34;D:\05_kuisu\project\06_stitchQC\Y00038K5\images\Y00038K5\Y00038K5_0008_0007_2022-12-30_16-10-15-066.tif&#34;

    dst_img = cv.imread(dst_path, 0)
    src_img = cv.imread(src_path, 0)

    match = FFTMatcher()
    match.horizontal = False
    match.overlap = 0.12
    result0 = match.neighbor_match(dst_img, src_img)
    # result = match.neighbor_match_v1(dst_img, src_img)
    print(&#39;ok&#39;)
    pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cellbin.contrib.fov_aligner.FFTMatcher"><code class="flex name class">
<span>class <span class="ident">FFTMatcher</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FFTMatcher(Matcher):
    def __init__(self):
        super(FFTMatcher, self).__init__()

    def pcm_peak(self, image1: np.ndarray, image2: np.ndarray):
        &#34;&#34;&#34;
        Compute peak correlation matrix for two images and Interpret the translation to find the translation with heighest ncc
        :param image1:the first image (the dimension must be 2)
        :param image2:the second image (the dimension must be 2)
        :return:
        _ncc : the highest ncc
        x : the selected x position
        y : the selected y position
        sub_image1: the overlapping area of image1
        sub_image2: the overlapping area of image2
        &#34;&#34;&#34;
        assert image1.ndim == 2, &#34;the dimension must be 2&#34;
        assert image2.ndim == 2, &#34;the dimension must be 2&#34;
        size_y, size_x = image1.shape[:2]
        pcm = self._pcm(image1, image2).real
        yins, xins, pcm_ = self._multi_peak_max(pcm)
        lims = np.array([[-size_y, size_y], [-size_x, size_x]])
        max_peak = self._interpret_translation(
            image1, image2, yins, xins, *lims[0], *lims[1]
        )  # 与输入进来的位置参数刚好相反
        ncc, offset_y, offset_x, sub_dst, sub_src = max_peak
        return ncc, offset_y, offset_x, sub_dst, sub_src

    def neighbor_match(self, src_img, dst_img, axis=0):
        &#34;&#34;&#34;
        Calculate the relative stitch offsets of adjacent FOVs
        :param src_img: [NxM], image of source, A is number of image to calculate cross-power specturm
        :param dst_img:[NxM], image of destination
        :return: offset, confidence
        &#34;&#34;&#34;
        if axis == 0:
            self.horizontal = True
        else: self.horizontal = False

        src_img, dst_img = self.get_roi(src_img, dst_img)
        ncc, offset_y, offset_x, sub_dst, sub_src = self.pcm_peak(src_img, dst_img)
        # Local image confirmation
        if sub_dst is not None:
            sub_dst_crop, sub_src_crop = self._slice_images(sub_dst, sub_src)
        else:
            return None
        if len(sub_src_crop) &gt; 0:
            (y0, x0), confidence0, fft_image = self._stack_peak_pc(sub_dst_crop, sub_src_crop)
            offset_x += x0
            offset_y += y0
        else:
            confidence0 = ncc

        size_y, size_x = dst_img.shape[:2]
        if self.horizontal:
            offset_x = offset_x - size_x
        else:
            offset_y = offset_y - size_y

        return [offset_x, offset_y, confidence0 * 100, ncc * 100]

    @staticmethod
    def ncc(image1, image2):
        &#34;&#34;&#34;Compute the normalized cross correlation for two images.
        Parameters
        ---------
        image1 : the first image (the dimension must be 2)
        image2 : the second image (the dimension must be 2)
        Returns
        -------
        ncc : the normalized cross correlation
        &#34;&#34;&#34;
        assert image1.ndim == 2
        assert image2.ndim == 2
        assert np.array_equal(image1.shape, image2.shape)
        image1 = image1.flatten()
        image2 = image2.flatten()
        n = np.dot(image1 - np.mean(image1), image2 - np.mean(image2))
        d = np.linalg.norm(image1) * np.linalg.norm(image2)
        if d &lt; 0.0001:
            return 0
        return n / d

    @staticmethod
    def extract_overlap_subregion(image, y: int, x: int):
        &#34;&#34;&#34;Extract the overlapping subregion of the image.
        Parameters
        ---------
        image : the image (the dimension must be 2)
        y : the y (second last dim.) position
        x : the x (last dim.) position
        Returns
        subimage : the extracted subimage
        &#34;&#34;&#34;
        sizeY = image.shape[0]
        sizeX = image.shape[1]
        assert (np.abs(y) &lt; sizeY) and (np.abs(x) &lt; sizeX)
        # clip x to (0, size_Y)
        h_start = int(max(0, min(y, sizeY, key=int), key=int))
        # clip x+sizeY to (0, size_Y)
        h_end = int(max(0, min(y + sizeY, sizeY, key=int), key=int))
        w_start = int(max(0, min(x, sizeX, key=int), key=int))
        w_end = int(max(0, min(x + sizeX, sizeX, key=int), key=int))
        return image[h_start:h_end, w_start:w_end]

    def _slice_images(self, src_img, dst_img):
        &#34;&#34;&#34;
        crop and stack images
        :param src_img: template image
        :param dst_img: background image
        :return: flod images
        &#34;&#34;&#34;
        ratio = 0.7
        overlap_size = min(dst_img.shape[:2])
        width = overlap_size if self.horizontal else int(overlap_size * ratio)
        height = int(overlap_size * ratio) if self.horizontal else overlap_size
        src_slice_img = self.slice_image(src_img, slice_width=width, slice_height=height,
                                         overlap_height_ratio=0.2, overlap_width_ratio=0.2, )
        dst_slice_img = self.slice_image(dst_img, slice_width=width, slice_height=height,
                                         overlap_height_ratio=0.2, overlap_width_ratio=0.2, )
        src_slice_img, dst_slice_img = self._fliter_null_information_image(src_slice_img, dst_slice_img)
        return np.array(src_slice_img), np.array(dst_slice_img)

    def _stack_peak_pc(self, stack_im0, stack_im1):
        &#34;&#34;&#34;
        Computes phase correlation between stack im0 and stack im1 and Interpret the translation to find the translation
        Args:
            stack_im0: the first stack image, N*W*H
            stack_im1: the second stack image, N*W*H
        Returns:
            ret: location
            success: matcher degree
            scps_result: phase correlation image
        &#34;&#34;&#34;
        # TODO: Implement some form of high-pass filtering of PHASE correlation
        stack_im0 = [self._apodize(im) for im in stack_im0]
        stack_im1 = [self._apodize(im) for im in stack_im1]
        f0, f1 = [np.fft.fft2(arr) for arr in (stack_im0, stack_im1)]
        # spectrum can be filtered (already),
        # so we have to take precaution against dividing by 0
        eps = abs(f1).max() * 1e-15
        cps = abs(np.fft.ifft2((f0 * f1.conjugate()) / (abs(f0) * abs(f1) + eps)))
        # scps = shifted cps
        scps = np.fft.fftshift(cps)
        scps_result = np.zeros_like(scps[0])
        sum_weights = 0 + 0.00000001
        for i, scp in enumerate(scps):
            scp = cv.GaussianBlur(scp, (7, 7), 1)
            thread = np.percentile(scp, 95)
            scp[np.where(scp &lt; thread)] = 0
            (_, _), success = self._argmax_translation(scp)
            scps_result += scp * success
            sum_weights += success
        scps_result = scps_result / sum_weights
        (t0, t1), success = self._argmax_translation(scps_result)  # (y,x)
        if success &lt; 0.05:
            scps_result = cv.GaussianBlur(scps_result, (7, 7), 1)
            win = cv.createHanningWindow((7, 7), cv.CV_32F)
            scps_result = cv.filter2D(scps_result, -1, win)
            (t0, t1), _ = self._argmax_translation(scps_result)  # (y,x)

        ret = np.array((np.round(t0), np.round(t1)))

        # _compensate_fftshift is not appropriate here, this is OK.
        t0 -= scps[0].shape[0] // 2
        t1 -= scps[0].shape[1] // 2

        ret -= np.array(scps[0].shape, int) // 2
        return ret, success, scps_result

    def _interpret_translation(self,
                               image1: np.ndarray,
                               image2: np.ndarray,
                               yins: np.ndarray,
                               xins: np.ndarray,
                               ymin: int,
                               ymax: int,
                               xmin: int,
                               xmax: int,
                               n: int = 2,
                               ):
        &#34;&#34;&#34;Interpret the translation to find the translation with heighest ncc.
        Parameters
        ---------
        image1 : the first image (the dimension must be 2)
        image2 : the second image (the dimension must be 2)
        yins : the y positions estimated by PCM
        xins : the x positions estimated by PCM
        ymin : the minimum value of y (second last dim.)
        ymax : the maximum value of y (second last dim.)
        xmin : the minimum value of x (last dim.)
        xmax : the maximum value of x (last dim.)
        n : the number of peaks to check, default is 2.
        Returns
        _ncc : the highest ncc
        x : the selected x position
        y : the selected y position
        &#34;&#34;&#34;
        assert image1.ndim == 2
        assert image2.ndim == 2
        assert np.array_equal(image1.shape, image2.shape)
        sizeY = image1.shape[0]
        sizeX = image1.shape[1]
        assert np.all(0 &lt;= yins) and np.all(yins &lt; sizeY)
        assert np.all(0 &lt;= xins) and np.all(xins &lt; sizeX)

        _ncc = -np.infty
        x, y = 0, 0
        overlap_img_1 = None
        overlap_img_2 = None

        ymagss = [yins, sizeY - yins]
        ymagss[1][ymagss[0] == 0] = 0
        xmagss = [xins, sizeX - xins]
        xmagss[1][xmagss[0] == 0] = 0

        # concatenate all the candidates
        _poss = []
        for ymags, xmags, ysign, xsign in itertools.product(
                ymagss, xmagss, [-1, +1], [-1, +1]
        ):
            yvals = ymags * ysign
            xvals = xmags * xsign
            _poss.append([yvals, xvals])
        poss = np.array(_poss)
        valid_ind = (
                (ymin &lt;= poss[:, 0, :])
                &amp; (poss[:, 0, :] &lt;= ymax)
                &amp; (xmin &lt;= poss[:, 1, :])
                &amp; (poss[:, 1, :] &lt;= xmax)
        )
        assert np.any(valid_ind)
        valid_ind = np.any(valid_ind, axis=0)
        for pos in np.moveaxis(poss[:, :, valid_ind], -1, 0)[: int(n)]:
            for yval, xval in pos:
                if (ymin &lt;= yval) and (yval &lt;= ymax) and (xmin &lt;= xval) and (xval &lt;= xmax):
                    subI1 = self.extract_overlap_subregion(image1, yval, xval)
                    subI2 = self.extract_overlap_subregion(image2, -yval, -xval)
                    if subI1.size / (sizeX * sizeY) &gt; 0.05 and min(subI1.shape) &gt; 80:  # 在overlap区域, 最少10%的重叠区域
                        ncc_val = self.ncc(subI1, subI2)
                        if ncc_val &gt; _ncc:
                            _ncc = float(ncc_val)
                            y = int(yval)
                            x = int(xval)
                            overlap_img_1 = subI1
                            overlap_img_2 = subI2
        return _ncc, y, x, overlap_img_1, overlap_img_2

    def _get_success(self, array, coord, radius=2):
        &#34;&#34;&#34;
        Given a coord, examine the array around it and return a number signifying
        how good is the &#34;match&#34;.

        Args:
            radius: Get the success as a sum of neighbor of coord of this radius
            coord: Coordinates of the maximum. Float numbers are allowed
                (and converted to int inside)

        Returns:
            Success as float between 0 and 1 (can get slightly higher than 1).
            The meaning of the number is loose, but the higher the better.
        &#34;&#34;&#34;
        coord = np.round(coord).astype(int)
        coord = tuple(coord)

        subarr = self._get_subarr(array, coord, radius)

        theval = subarr.sum()
        theval2 = array[coord]
        # bigval = np.percentile(array, 97)
        # success = theval / bigval
        # TODO: Think this out
        success = np.sqrt(theval * theval2)
        return success

    def _argmax_translation(self, array):
        # We want to keep the original and here is obvious that
        # it won&#39;t get changed inadvertently
        array_orig = array.copy()
        ashape = np.array(array.shape, int)
        mask = np.ones(ashape, float)
        array *= mask

        # WE ARE FFTSHIFTED already.
        # ban translations that are too big
        aporad = (ashape // 6).min()
        mask2 = self._get_apofield(ashape, aporad, corner=True)
        array *= mask2
        # Find what we look for
        tvec = self._argmax_ext(array, &#39;inf&#39;)
        tvec = self._interpolate(array_orig, tvec)

        # If we use constraints or min filter,
        # array_orig[tvec] may not be the maximum
        success = self._get_success(array_orig, tuple(tvec), 2)

        return tvec, success

    def _interpolate(self, array, rough, rad=2):
        &#34;&#34;&#34;
        Returns index that is in the array after being rounded.

        The result index tuple is in each of its components between zero and the
        array&#39;s shape.
        &#34;&#34;&#34;
        rough = np.round(rough).astype(int)
        surroundings = self._get_subarr(array, rough, rad)
        com = self._argmax_ext(surroundings, 1)
        offset = com - rad
        ret = rough + offset
        # similar to win.wrap, so
        # -0.2 becomes 0.3 and then again -0.2, which is rounded to 0
        # -0.8 becomes - 0.3 -&gt; len() - 0.3 and then len() - 0.8,
        # which is rounded to len() - 1. Yeah!
        ret += 0.5
        ret %= np.array(array.shape).astype(int)
        ret -= 0.5
        return ret

    def _apodize(self, what, aporad=None):
        &#34;&#34;&#34;
        Given an image, it apodizes it (so it becomes quasi-seamless).
        When ``ratio`` is None, color near the edges will converge
        to the same colour, whereas when ratio is a float number, a blurred
        original image will serve as background.

        Args:
            what: The original image
            aporad (int): Radius [px], width of the band near the edges
                that will get modified
        Returns:
            The apodized image
        &#34;&#34;&#34;
        if aporad is None:
            mindim = min(what.shape)
            aporad = int(mindim * 0.12)
        apofield = self._get_apofield(what.shape, aporad)
        res = what * apofield
        bg = self._get_borderval(what, aporad // 2)
        res += bg * (1 - apofield)
        return res

    @staticmethod
    def _get_borderval(img, radius=None):
        &#34;&#34;&#34;
        Given an image and a radius, examine the average value of the image
        at most radius pixels from the edge
        &#34;&#34;&#34;
        if radius is None:
            mindim = min(img.shape)
            radius = max(1, mindim // 20)
        mask = np.zeros_like(img, dtype=bool)
        mask[:, :radius] = True
        mask[:, -radius:] = True
        mask[radius, :] = True
        mask[-radius:, :] = True

        mean = np.median(img[mask])
        return mean

    @staticmethod
    def _get_apofield(shape, aporad, corner=False):
        &#34;&#34;&#34;
        Returns an array between 0 and 1 that goes to zero close to the edges.
        &#34;&#34;&#34;
        if aporad == 0:
            return np.ones(shape, dtype=float)
        apos = np.hanning(aporad * 2)
        vecs = []
        for dim in shape:
            assert dim &gt; aporad * 2, \
                &#34;Apodization radius %d too big for shape dim. %d&#34; % (aporad, dim)
            toapp = np.ones(dim)
            toapp[:aporad] = apos[:aporad]
            toapp[-aporad:] = apos[-aporad:]
            vecs.append(toapp)
        apofield = np.outer(vecs[0], vecs[1])
        if corner:
            apofield[aporad:-aporad] = 1
            apofield[:, aporad:-aporad] = 1

        return apofield

    @staticmethod
    def _multi_peak_max(PCM):
        &#34;&#34;&#34;Find the first to n th largest peaks in PCM.
        PCM : the peak correlation matrix
        Returns
        rows : the row indices for the peaks
        cols : the column indices for the peaks
        vals : the values of the peaks
        &#34;&#34;&#34;
        row, col = np.unravel_index(np.argsort(PCM.ravel()), PCM.shape)
        vals = PCM[row[::-1], col[::-1]]
        return row[::-1], col[::-1], vals

    @staticmethod
    def _pcm(image1, image2):
        &#34;&#34;&#34;Compute peak correlation matrix for two images.
        image1 :  the first image (the dimension must be 2)
        image2 : the second image (the dimension must be 2)
        Returns
        PCM : the peak correlation matrix
        &#34;&#34;&#34;
        assert image1.ndim == 2
        assert image2.ndim == 2
        assert np.array_equal(image1.shape, image2.shape)
        F1 = np.fft.fft2(image1)
        F2 = np.fft.fft2(image2)
        FC = F1 * np.conjugate(F2)
        return np.fft.ifft2(FC / (np.abs(FC) + 0.0000000001)).real.astype(np.float32)

    @staticmethod
    def _fliter_null_information_image(src_imgs, dst_imgs):
        &#34;&#34;&#34;
        Filter out sub img that has no information
        :param src_imgs: first crop and stack images
        :param dst_imgs: second crop and stack images
        :return: valid sub images
        &#34;&#34;&#34;
        src_lists = []
        dst_lists = []
        fov_deepth = 8 if src_imgs[0].dtype == np.uint8 else 16
        img_grad_thread = 0.08 * (2 ** fov_deepth / 256) + 0.12
        for src, dst in zip(src_imgs, dst_imgs):
            src_grad = cv.Sobel(src, -1, 1, 1).std()
            dst_grad = cv.Sobel(dst, -1, 1, 1).std()
            if max(src_grad, dst_grad) &gt; img_grad_thread:
                src_lists.append(src)
                dst_lists.append(dst)
        return src_lists, dst_lists

    @staticmethod
    def _argmax_ext(array, exponent):
        &#34;&#34;&#34;
        Calculate coordinates of the COM (center of mass) of the provided array.
        Args:
            array (ndarray): The array to be examined.
            exponent (float or &#39;inf&#39;): The exponent we power the array with. If the
                value &#39;inf&#39; is given, the coordinage of the array maximum is taken.
        Returns:
            np.ndarray: The COM coordinate tuple, float values are allowed!
        &#34;&#34;&#34;

        # When using an integer exponent for _argmax_ext, it is good to have the
        # neutral rotation/scale in the center rather near the edges
        if exponent == &#34;inf&#34;:
            amax = np.argmax(array)
            ret = list(np.unravel_index(amax, array.shape))
        else:
            col = np.arange(array.shape[0])[:, np.newaxis]
            row = np.arange(array.shape[1])[np.newaxis, :]

            arr2 = array ** exponent
            arrsum = arr2.sum()
            if arrsum == 0:
                # We have to return SOMETHING, so let&#39;s go for (0, 0)
                return np.zeros(2)
            arrprody = np.sum(arr2 * col) / arrsum
            arrprodx = np.sum(arr2 * row) / arrsum
            ret = [arrprody, arrprodx]
            # We don&#39;t use it, but it still tells us about value distribution

        return np.array(ret)

    @staticmethod
    def _get_subarr(array, center, rad):
        &#34;&#34;&#34;
        Args:
            array (ndarray): The array to search
            center (2-tuple): The point in the array to search around
            rad (int): Search radius, no radius (i.e. get the single point)
                implies rad == 0
        &#34;&#34;&#34;
        dim = 1 + 2 * rad
        subarr = np.zeros((dim,) * 2)
        corner = np.array(center) - rad
        for ii in range(dim):
            yidx = corner[0] + ii
            yidx %= array.shape[0]
            for jj in range(dim):
                xidx = corner[1] + jj
                xidx %= array.shape[1]
                subarr[ii, jj] = array[yidx, xidx]
        return subarr

    # @staticmethod
    # def _show_3d(image):
    #     from matplotlib import cm
    #     import matplotlib.pyplot as plt
    #     h, w = image.shape
    #     X = np.arange(0, w, 1)
    #     Y = np.arange(0, h, 1)
    #     X, Y = np.meshgrid(X, Y)
    #     fig = plt.figure()
    #     ax = fig.add_subplot(projection=&#39;3d&#39;)
    #     ax.plot_surface(X, Y, image, cmap=cm.coolwarm)
    #     plt.show()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="cellbin.contrib.fov_aligner.Matcher" href="#cellbin.contrib.fov_aligner.Matcher">Matcher</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.FFTMatcher.extract_overlap_subregion"><code class="name flex">
<span>def <span class="ident">extract_overlap_subregion</span></span>(<span>image, y: int, x: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract the overlapping subregion of the image.
Parameters</p>
<hr>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>the image (the dimension must be 2)</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>the y (second last dim.) position</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>the x (last dim.) position</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>Returns</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>subimage</code></strong> :&ensp;<code>the extracted subimage</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def extract_overlap_subregion(image, y: int, x: int):
    &#34;&#34;&#34;Extract the overlapping subregion of the image.
    Parameters
    ---------
    image : the image (the dimension must be 2)
    y : the y (second last dim.) position
    x : the x (last dim.) position
    Returns
    subimage : the extracted subimage
    &#34;&#34;&#34;
    sizeY = image.shape[0]
    sizeX = image.shape[1]
    assert (np.abs(y) &lt; sizeY) and (np.abs(x) &lt; sizeX)
    # clip x to (0, size_Y)
    h_start = int(max(0, min(y, sizeY, key=int), key=int))
    # clip x+sizeY to (0, size_Y)
    h_end = int(max(0, min(y + sizeY, sizeY, key=int), key=int))
    w_start = int(max(0, min(x, sizeX, key=int), key=int))
    w_end = int(max(0, min(x + sizeX, sizeX, key=int), key=int))
    return image[h_start:h_end, w_start:w_end]</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.FFTMatcher.ncc"><code class="name flex">
<span>def <span class="ident">ncc</span></span>(<span>image1, image2)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the normalized cross correlation for two images.
Parameters</p>
<hr>
<dl>
<dt><strong><code>image1</code></strong> :&ensp;<code>the first image (the dimension must be 2)</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>image2</code></strong> :&ensp;<code>the second image (the dimension must be 2)</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ncc</code></strong> :&ensp;<code>the normalized cross correlation</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def ncc(image1, image2):
    &#34;&#34;&#34;Compute the normalized cross correlation for two images.
    Parameters
    ---------
    image1 : the first image (the dimension must be 2)
    image2 : the second image (the dimension must be 2)
    Returns
    -------
    ncc : the normalized cross correlation
    &#34;&#34;&#34;
    assert image1.ndim == 2
    assert image2.ndim == 2
    assert np.array_equal(image1.shape, image2.shape)
    image1 = image1.flatten()
    image2 = image2.flatten()
    n = np.dot(image1 - np.mean(image1), image2 - np.mean(image2))
    d = np.linalg.norm(image1) * np.linalg.norm(image2)
    if d &lt; 0.0001:
        return 0
    return n / d</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.FFTMatcher.neighbor_match"><code class="name flex">
<span>def <span class="ident">neighbor_match</span></span>(<span>self, src_img, dst_img, axis=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the relative stitch offsets of adjacent FOVs
:param src_img: [NxM], image of source, A is number of image to calculate cross-power specturm
:param dst_img:[NxM], image of destination
:return: offset, confidence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neighbor_match(self, src_img, dst_img, axis=0):
    &#34;&#34;&#34;
    Calculate the relative stitch offsets of adjacent FOVs
    :param src_img: [NxM], image of source, A is number of image to calculate cross-power specturm
    :param dst_img:[NxM], image of destination
    :return: offset, confidence
    &#34;&#34;&#34;
    if axis == 0:
        self.horizontal = True
    else: self.horizontal = False

    src_img, dst_img = self.get_roi(src_img, dst_img)
    ncc, offset_y, offset_x, sub_dst, sub_src = self.pcm_peak(src_img, dst_img)
    # Local image confirmation
    if sub_dst is not None:
        sub_dst_crop, sub_src_crop = self._slice_images(sub_dst, sub_src)
    else:
        return None
    if len(sub_src_crop) &gt; 0:
        (y0, x0), confidence0, fft_image = self._stack_peak_pc(sub_dst_crop, sub_src_crop)
        offset_x += x0
        offset_y += y0
    else:
        confidence0 = ncc

    size_y, size_x = dst_img.shape[:2]
    if self.horizontal:
        offset_x = offset_x - size_x
    else:
        offset_y = offset_y - size_y

    return [offset_x, offset_y, confidence0 * 100, ncc * 100]</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.FFTMatcher.pcm_peak"><code class="name flex">
<span>def <span class="ident">pcm_peak</span></span>(<span>self, image1: numpy.ndarray, image2: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute peak correlation matrix for two images and Interpret the translation to find the translation with heighest ncc
:param image1:the first image (the dimension must be 2)
:param image2:the second image (the dimension must be 2)
:return:
_ncc : the highest ncc
x : the selected x position
y : the selected y position
sub_image1: the overlapping area of image1
sub_image2: the overlapping area of image2</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pcm_peak(self, image1: np.ndarray, image2: np.ndarray):
    &#34;&#34;&#34;
    Compute peak correlation matrix for two images and Interpret the translation to find the translation with heighest ncc
    :param image1:the first image (the dimension must be 2)
    :param image2:the second image (the dimension must be 2)
    :return:
    _ncc : the highest ncc
    x : the selected x position
    y : the selected y position
    sub_image1: the overlapping area of image1
    sub_image2: the overlapping area of image2
    &#34;&#34;&#34;
    assert image1.ndim == 2, &#34;the dimension must be 2&#34;
    assert image2.ndim == 2, &#34;the dimension must be 2&#34;
    size_y, size_x = image1.shape[:2]
    pcm = self._pcm(image1, image2).real
    yins, xins, pcm_ = self._multi_peak_max(pcm)
    lims = np.array([[-size_y, size_y], [-size_x, size_x]])
    max_peak = self._interpret_translation(
        image1, image2, yins, xins, *lims[0], *lims[1]
    )  # 与输入进来的位置参数刚好相反
    ncc, offset_y, offset_x, sub_dst, sub_src = max_peak
    return ncc, offset_y, offset_x, sub_dst, sub_src</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="cellbin.contrib.fov_aligner.Matcher" href="#cellbin.contrib.fov_aligner.Matcher">Matcher</a></b></code>:
<ul class="hlist">
<li><code><a title="cellbin.contrib.fov_aligner.Matcher.get_roi" href="#cellbin.contrib.fov_aligner.Matcher.get_roi">get_roi</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.Matcher.slice_image" href="#cellbin.contrib.fov_aligner.Matcher.slice_image">slice_image</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="cellbin.contrib.fov_aligner.FOVAligner"><code class="flex name class">
<span>class <span class="ident">FOVAligner</span></span>
<span>(</span><span>images_path, rows, cols, mode='FFT', multi=True, channel=0)</span>
</code></dt>
<dd>
<div class="desc"><p>计算出所有图像的偏移量</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FOVAligner(object):
    &#39;&#39;&#39;
    计算出所有图像的偏移量
    &#39;&#39;&#39;
    def __init__(self, images_path, rows, cols, mode=&#39;FFT&#39;, multi=True, channel=0):
        self.set_size(rows, cols)
        self.set_images_path(images_path)
        self.set_channel(channel)
        self.multi = multi # TODO 多进程选项
        self.matcher = FFTMatcher() if mode == &#39;FFT&#39; else SIFTMatcher()
        self.num = 5
        self.__init_value = 999

        self.horizontal_jitter = np.zeros((self.rows, self.cols, 2), dtype=int) + self.__init_value
        self.vertical_jitter = np.zeros((self.rows, self.cols, 2), dtype=int) + self.__init_value
        self.fov_mask = np.zeros((self.rows, self.cols, 3))

    def set_channel(self, channel):
        assert isinstance(channel, int), &#34;Channel type error.&#34;
        self.channel = channel

    def set_images_path(self, images_path):
        # assert isinstance(images_path, dict), &#34;Image path type error.&#34;
        self.images_path = images_path

    def set_size(self, rows, cols):
        assert isinstance(rows, int) or isinstance(rows, float), &#34;Rows type error.&#34;
        assert isinstance(cols, int) or isinstance(cols, float), &#34;Cols type error.&#34;
        self.rows = rows
        self.cols = cols

    def set_process(self, p):
        self.num = p
        if self.num &lt;= 1:
            self.multi = False  # @lizepeng add on 2023/05/17

    def create_jitter(self):
        &#39;&#39;&#39;
        Scan image and create offset.
        &#39;&#39;&#39;
        confi_mask = np.zeros((self.rows, self.cols, 2)) - 1
        ncc_confi_mask = np.zeros((self.rows, self.cols, 2)) - 1

        if self.multi:
            tesk_list = list()
            for row_index in range(self.rows):
                tesk_list.append({&#39;row&#39;: row_index})
            for col_index in range(self.cols):
                tesk_list.append({&#39;col&#39;: col_index})

            with Manager() as manager:
                h_j = manager.dict()
                v_j = manager.dict()
                confi_h = manager.dict()
                confi_v = manager.dict()
                # process_list = []
                # num = int(cpu_count() / 2)
                # num = 10
                clog.info(f&#34;stitch using {self.num} process&#34;)
                pool = Pool(processes=self.num)

                for tesk in tesk_list:
                    if list(tesk.keys())[0] == &#39;row&#39;:
                        row_index = tesk[&#39;row&#39;]
                        pool.apply_async(func=self._multi_jitter, args=(h_j, confi_h, row_index, None, 0))

                    elif list(tesk.keys())[0] == &#39;col&#39;:
                        col_index = tesk[&#39;col&#39;]
                        pool.apply_async(func=self._multi_jitter, args=(v_j, confi_v, None, col_index, 1))

                    else:
                        pass

                pool.close()
                pool.join()

                for key in h_j.keys():
                    row, col = [int(i) for i in key.split(&#39;_&#39;)]
                    self.horizontal_jitter[row, col, :] = h_j[key]
                    confi_mask[row, col, 0] = confi_h[key]

                for key in v_j.keys():
                    row, col = [int(i) for i in key.split(&#39;_&#39;)]
                    self.vertical_jitter[row, col, :] = v_j[key]
                    confi_mask[row, col, 1] = confi_v[key]
        else:
            clog.info(&#34;Scan Row by Row.&#34;)
            for i in tqdm(range(self.rows), desc=&#39;RowByRow&#39;):
                train = self._get_image(i, 0)
                for j in range(1, self.cols):
                    query = self._get_image(i, j)
                    if (train is not None) and (query is not None):
                        if np.max(train) == 0 or np.max(query) == 0: b = None
                        elif train.shape == query.shape:
                            try: b = self.matcher.neighbor_match(train, query, 0)
                            except: b = None
                        else: b = None
                        if b is not None:
                            self.horizontal_jitter[i, j, :] = b[:2]
                            confi_mask[i, j, 0] = b[2]
                            ncc_confi_mask[i, j, 0] = b[3]
                    train = query

            clog.info(&#34;Scan Col by Col.&#34;)
            for j in tqdm(range(self.cols), desc=&#39;ColByCol&#39;):
                train = self._get_image(0, j)
                for i in range(1, self.rows):
                    query = self._get_image(i, j)
                    if (train is not None) and (query is not None):
                        if np.max(train) == 0 or np.max(query) == 0: b = None
                        elif train.shape == query.shape:
                            try: b = self.matcher.neighbor_match(train, query, 1)
                            except: b = None
                        else: b = None
                        if b is not None:
                            self.vertical_jitter[i, j, :] = b[:2]
                            confi_mask[i, j, 1] = b[2]
                            ncc_confi_mask[i, j, 1] = b[3]
                    train = query

        self.horizontal_jitter, confi_mask[:, :, 0], _ = self.filter_abnormal_offset(self.horizontal_jitter,
                                                                                     confi_mask[:, :, 0],
                                                                                     thread=5)

        self.vertical_jitter, confi_mask[:, :, 1], _ = self.filter_abnormal_offset(self.vertical_jitter,
                                                                                   confi_mask[:, :, 1],
                                                                                   thread=5)

        self.fov_mask[:, :, :2] = confi_mask
        self.fov_mask[:, :, 2] = np.max(confi_mask, axis=2)
        self.fov_mask[:, :, 2][np.where(self.fov_mask[:, :, 2] &gt;= 99)] = 99
        clog.info(&#34;Scan image done!&#34;)

    def _offset_eval(self, height, width, overlap=0.1):
        &#34;&#34;&#34;
        :param height: fov image height
        :param width: fov image width
        :param overlap:
        :return: offset matrix r * c * 1
        &#34;&#34;&#34;
        h_j = copy.deepcopy(self.horizontal_jitter)
        v_j = copy.deepcopy(self.vertical_jitter)

        for r in range(self.rows):
            for c in range(self.cols):
                if h_j[r, c, 0] != -self.__init_value and \
                        h_j[r, c, 0] != self.__init_value:
                    h_j[r, c, 0] += int(width * overlap)
                if v_j[r, c, 1] != -self.__init_value and \
                        v_j[r, c, 1] != self.__init_value:
                    v_j[r, c, 1] += int(height * overlap)

        h_e = (h_j[:, :, 0] ** 2 + h_j[:, :, 1] ** 2) ** 0.5
        h_e[np.where(h_e &gt;= self.__init_value)] = -1

        v_e = (v_j[:, :, 0] ** 2 + v_j[:, :, 1] ** 2) ** 0.5
        v_e[np.where(v_e &gt;= self.__init_value)] = -1

        e = np.max(np.stack((h_e.astype(int), v_e.astype(int)), axis=2), axis=2)

        return e, h_e, v_e

    def _multi_jitter(self, jitter, confi, row=None, col=None, axis=0):
        if axis == 0:
            train = self._get_image(row, 0)
            for col_index in range(1, self.cols):
                query = self._get_image(row, col_index)
                if (train is not None) and (query is not None):
                    if np.max(train) == 0 or np.max(query) == 0: b = None
                    elif train.shape == query.shape:
                        try: b = self.matcher.neighbor_match(train, query, 0)
                        except: b = None
                    else: b = None
                    jitter[rc_key(row, col_index)] = b[:2]
                    confi[rc_key(row, col_index)] = b[2]
                train = query

        elif axis == 1:
            train = self._get_image(0, col)
            for row_index in range(1, self.rows):
                query = self._get_image(row_index, col)
                if (train is not None) and (query is not None):
                    if np.max(train) == 0 or np.max(query) == 0: b = None
                    elif train.shape == query.shape:
                        try: b = self.matcher.neighbor_match(train, query, 1)
                        except: b = None
                    else: b = None
                    jitter[rc_key(row_index, col)] = b[:2]
                    confi[rc_key(row_index, col)] = b[2]
                train = query
        else:
            clog.info(&#34;axis value error.&#34;)

    def _get_image(self, row, col):
        &#34;&#34;&#34;
        row: fov in row
        col: fov in col
        &#34;&#34;&#34;
        temp_index = list(self.images_path.keys())[0]
        if len(temp_index.split(&#39;_&#39;)[0]) == 4:
            key = rc_key(row, col)
        else: key = rc_key(row, col, source=True)
        if type(self.images_path) is dict:
            if key not in self.images_path.keys():
                return None
            img = Image()
            img.read(self.images_path[key])
            arr = img.image
            if arr.ndim == 3:
                arr = arr[:, :, self.channel]
            return arr

    @staticmethod
    def filter_abnormal_offset(offset: np.array, stitch_confi_mask: np.ndarray, thread: int = 2):
        &#34;&#34;&#34;
        过滤异常的offset
        :param offset: W*H*C
        :return:
        &#34;&#34;&#34;
        c = offset.shape[2]
        max_thread = 0
        for i in range(c):
            offset_1 = offset[:, :, i]  # 所有offset
            valid_offset = offset_1[np.where((offset_1 != -999) &amp; (stitch_confi_mask &gt; thread))]
            # 去掉最大值和最小值 取出[0.2, 0.8]之间的值来求均值和方差
            if len(valid_offset) &gt; 3:
                percentile = np.percentile(valid_offset, (25, 50, 75), interpolation=&#39;midpoint&#39;)
                Q1, Q3 = percentile[0], percentile[2]
                IQR = Q3 - Q1
                IQR = IQR if IQR &gt; 5 else IQR + 3

                thread_max = Q3 + 1.5 * IQR
                thread_min = Q1 - 1.5 * IQR

                error_fov = np.zeros(shape=(0, 2), dtype=np.int32)
                max_error_fov = np.array(np.where((offset_1 &gt; thread_max))).T
                min_error_fov = np.array(np.where((offset_1 &lt; thread_min) &amp; (offset_1 != -999))).T
                error_fov = np.vstack([error_fov, max_error_fov, min_error_fov])
                if len(error_fov) &gt; 0:
                    for row, col in error_fov:
                        if stitch_confi_mask[row, col] &gt; thread * 3:  # 置信度高, 阈值放宽
                            thread_max += IQR * 0.6
                            thread_min -= IQR * 0.6
                        if offset_1[row, col] &gt; thread_max or offset_1[row, col] &lt; thread_min:
                            offset[row, col] = -999
                            stitch_confi_mask[row, col] = 0

                if thread_max &gt; max_thread:
                    max_thread = thread_max
        return offset, stitch_confi_mask, max_thread</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.FOVAligner.filter_abnormal_offset"><code class="name flex">
<span>def <span class="ident">filter_abnormal_offset</span></span>(<span>offset: <built-in function array>, stitch_confi_mask: numpy.ndarray, thread: int = 2)</span>
</code></dt>
<dd>
<div class="desc"><p>过滤异常的offset
:param offset: W<em>H</em>C
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def filter_abnormal_offset(offset: np.array, stitch_confi_mask: np.ndarray, thread: int = 2):
    &#34;&#34;&#34;
    过滤异常的offset
    :param offset: W*H*C
    :return:
    &#34;&#34;&#34;
    c = offset.shape[2]
    max_thread = 0
    for i in range(c):
        offset_1 = offset[:, :, i]  # 所有offset
        valid_offset = offset_1[np.where((offset_1 != -999) &amp; (stitch_confi_mask &gt; thread))]
        # 去掉最大值和最小值 取出[0.2, 0.8]之间的值来求均值和方差
        if len(valid_offset) &gt; 3:
            percentile = np.percentile(valid_offset, (25, 50, 75), interpolation=&#39;midpoint&#39;)
            Q1, Q3 = percentile[0], percentile[2]
            IQR = Q3 - Q1
            IQR = IQR if IQR &gt; 5 else IQR + 3

            thread_max = Q3 + 1.5 * IQR
            thread_min = Q1 - 1.5 * IQR

            error_fov = np.zeros(shape=(0, 2), dtype=np.int32)
            max_error_fov = np.array(np.where((offset_1 &gt; thread_max))).T
            min_error_fov = np.array(np.where((offset_1 &lt; thread_min) &amp; (offset_1 != -999))).T
            error_fov = np.vstack([error_fov, max_error_fov, min_error_fov])
            if len(error_fov) &gt; 0:
                for row, col in error_fov:
                    if stitch_confi_mask[row, col] &gt; thread * 3:  # 置信度高, 阈值放宽
                        thread_max += IQR * 0.6
                        thread_min -= IQR * 0.6
                    if offset_1[row, col] &gt; thread_max or offset_1[row, col] &lt; thread_min:
                        offset[row, col] = -999
                        stitch_confi_mask[row, col] = 0

            if thread_max &gt; max_thread:
                max_thread = thread_max
    return offset, stitch_confi_mask, max_thread</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.FOVAligner.create_jitter"><code class="name flex">
<span>def <span class="ident">create_jitter</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Scan image and create offset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_jitter(self):
    &#39;&#39;&#39;
    Scan image and create offset.
    &#39;&#39;&#39;
    confi_mask = np.zeros((self.rows, self.cols, 2)) - 1
    ncc_confi_mask = np.zeros((self.rows, self.cols, 2)) - 1

    if self.multi:
        tesk_list = list()
        for row_index in range(self.rows):
            tesk_list.append({&#39;row&#39;: row_index})
        for col_index in range(self.cols):
            tesk_list.append({&#39;col&#39;: col_index})

        with Manager() as manager:
            h_j = manager.dict()
            v_j = manager.dict()
            confi_h = manager.dict()
            confi_v = manager.dict()
            # process_list = []
            # num = int(cpu_count() / 2)
            # num = 10
            clog.info(f&#34;stitch using {self.num} process&#34;)
            pool = Pool(processes=self.num)

            for tesk in tesk_list:
                if list(tesk.keys())[0] == &#39;row&#39;:
                    row_index = tesk[&#39;row&#39;]
                    pool.apply_async(func=self._multi_jitter, args=(h_j, confi_h, row_index, None, 0))

                elif list(tesk.keys())[0] == &#39;col&#39;:
                    col_index = tesk[&#39;col&#39;]
                    pool.apply_async(func=self._multi_jitter, args=(v_j, confi_v, None, col_index, 1))

                else:
                    pass

            pool.close()
            pool.join()

            for key in h_j.keys():
                row, col = [int(i) for i in key.split(&#39;_&#39;)]
                self.horizontal_jitter[row, col, :] = h_j[key]
                confi_mask[row, col, 0] = confi_h[key]

            for key in v_j.keys():
                row, col = [int(i) for i in key.split(&#39;_&#39;)]
                self.vertical_jitter[row, col, :] = v_j[key]
                confi_mask[row, col, 1] = confi_v[key]
    else:
        clog.info(&#34;Scan Row by Row.&#34;)
        for i in tqdm(range(self.rows), desc=&#39;RowByRow&#39;):
            train = self._get_image(i, 0)
            for j in range(1, self.cols):
                query = self._get_image(i, j)
                if (train is not None) and (query is not None):
                    if np.max(train) == 0 or np.max(query) == 0: b = None
                    elif train.shape == query.shape:
                        try: b = self.matcher.neighbor_match(train, query, 0)
                        except: b = None
                    else: b = None
                    if b is not None:
                        self.horizontal_jitter[i, j, :] = b[:2]
                        confi_mask[i, j, 0] = b[2]
                        ncc_confi_mask[i, j, 0] = b[3]
                train = query

        clog.info(&#34;Scan Col by Col.&#34;)
        for j in tqdm(range(self.cols), desc=&#39;ColByCol&#39;):
            train = self._get_image(0, j)
            for i in range(1, self.rows):
                query = self._get_image(i, j)
                if (train is not None) and (query is not None):
                    if np.max(train) == 0 or np.max(query) == 0: b = None
                    elif train.shape == query.shape:
                        try: b = self.matcher.neighbor_match(train, query, 1)
                        except: b = None
                    else: b = None
                    if b is not None:
                        self.vertical_jitter[i, j, :] = b[:2]
                        confi_mask[i, j, 1] = b[2]
                        ncc_confi_mask[i, j, 1] = b[3]
                train = query

    self.horizontal_jitter, confi_mask[:, :, 0], _ = self.filter_abnormal_offset(self.horizontal_jitter,
                                                                                 confi_mask[:, :, 0],
                                                                                 thread=5)

    self.vertical_jitter, confi_mask[:, :, 1], _ = self.filter_abnormal_offset(self.vertical_jitter,
                                                                               confi_mask[:, :, 1],
                                                                               thread=5)

    self.fov_mask[:, :, :2] = confi_mask
    self.fov_mask[:, :, 2] = np.max(confi_mask, axis=2)
    self.fov_mask[:, :, 2][np.where(self.fov_mask[:, :, 2] &gt;= 99)] = 99
    clog.info(&#34;Scan image done!&#34;)</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.FOVAligner.set_channel"><code class="name flex">
<span>def <span class="ident">set_channel</span></span>(<span>self, channel)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_channel(self, channel):
    assert isinstance(channel, int), &#34;Channel type error.&#34;
    self.channel = channel</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.FOVAligner.set_images_path"><code class="name flex">
<span>def <span class="ident">set_images_path</span></span>(<span>self, images_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_images_path(self, images_path):
    # assert isinstance(images_path, dict), &#34;Image path type error.&#34;
    self.images_path = images_path</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.FOVAligner.set_process"><code class="name flex">
<span>def <span class="ident">set_process</span></span>(<span>self, p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_process(self, p):
    self.num = p
    if self.num &lt;= 1:
        self.multi = False  # @lizepeng add on 2023/05/17</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.FOVAligner.set_size"><code class="name flex">
<span>def <span class="ident">set_size</span></span>(<span>self, rows, cols)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_size(self, rows, cols):
    assert isinstance(rows, int) or isinstance(rows, float), &#34;Rows type error.&#34;
    assert isinstance(cols, int) or isinstance(cols, float), &#34;Cols type error.&#34;
    self.rows = rows
    self.cols = cols</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="cellbin.contrib.fov_aligner.Matcher"><code class="flex name class">
<span>class <span class="ident">Matcher</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Matcher(object):

    def __init__(self):
        self.overlap = 0.1
        self.horizontal = True

    def get_roi(self, arr0, arr1):
        &#34;&#34;&#34;
        get overlap area
        :param arr0: fov 1
        :param arr1: fov 2
        :return: overlap area
        &#34;&#34;&#34;
        h0, w0 = arr0.shape
        h1, w1 = arr1.shape
        w = int(min(w0, w1) * self.overlap)
        h = int(min(h0, h1) * self.overlap)

        if self.horizontal:
            x0, y0, x1, y1 = [w0 - w, 0, w0, h0]
            x0_, y0_, x1_, y1_ = [0, 0, w, h1]
        else:
            x0, y0, x1, y1 = [0, h0 - h, w0, h0]
            x0_, y0_, x1_, y1_ = [0, 0, w1, h]
        return arr0[y0: y1, x0: x1], arr1[y0_: y1_, x0_: x1_]

    @staticmethod
    def slice_image(image: np.ndarray, slice_width: int, slice_height: int,
                    overlap_height_ratio=0.2, overlap_width_ratio=0.2):
        &#34;&#34;&#34;
        Image is cropped and stacked
        &#34;&#34;&#34;
        if image.ndim == 3:
            image_height, image_width, _ = image.shape
        else:
            image_height, image_width = image.shape

        y_overlap = int(overlap_height_ratio * slice_height)
        x_overlap = int(overlap_width_ratio * slice_width)

        slice_bboxes = []
        y_max = y_min = 0
        while y_max &lt; image_height:
            x_min = x_max = 0
            y_max = y_min + slice_height
            while x_max &lt; image_width:
                x_max = x_min + slice_width
                if y_max &gt; image_height or x_max &gt; image_width:
                    xmax = min(image_width, x_max)
                    ymax = min(image_height, y_max)
                    xmin = max(0, xmax - slice_width)
                    ymin = max(0, ymax - slice_height)
                    slice_bboxes.append([xmin, ymin, xmax, ymax])
                else:
                    slice_bboxes.append([x_min, y_min, x_max, y_max])
                x_min = x_max - x_overlap
            y_min = y_max - y_overlap

        n_ims = 0
        sliced_image_result = []
        assert len(slice_bboxes) &gt; 0, &#34;slice image must &gt; 0&#34;
        for slice_bbox in slice_bboxes:
            n_ims += 1
            # extract image
            tlx = slice_bbox[0]
            tly = slice_bbox[1]
            brx = slice_bbox[2]
            bry = slice_bbox[3]
            image_pil_slice = image[tly:bry, tlx:brx]
            sliced_image_result.append(image_pil_slice)
        return sliced_image_result</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="cellbin.contrib.fov_aligner.FFTMatcher" href="#cellbin.contrib.fov_aligner.FFTMatcher">FFTMatcher</a></li>
<li><a title="cellbin.contrib.fov_aligner.SIFTMatcher" href="#cellbin.contrib.fov_aligner.SIFTMatcher">SIFTMatcher</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.Matcher.slice_image"><code class="name flex">
<span>def <span class="ident">slice_image</span></span>(<span>image: numpy.ndarray, slice_width: int, slice_height: int, overlap_height_ratio=0.2, overlap_width_ratio=0.2)</span>
</code></dt>
<dd>
<div class="desc"><p>Image is cropped and stacked</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def slice_image(image: np.ndarray, slice_width: int, slice_height: int,
                overlap_height_ratio=0.2, overlap_width_ratio=0.2):
    &#34;&#34;&#34;
    Image is cropped and stacked
    &#34;&#34;&#34;
    if image.ndim == 3:
        image_height, image_width, _ = image.shape
    else:
        image_height, image_width = image.shape

    y_overlap = int(overlap_height_ratio * slice_height)
    x_overlap = int(overlap_width_ratio * slice_width)

    slice_bboxes = []
    y_max = y_min = 0
    while y_max &lt; image_height:
        x_min = x_max = 0
        y_max = y_min + slice_height
        while x_max &lt; image_width:
            x_max = x_min + slice_width
            if y_max &gt; image_height or x_max &gt; image_width:
                xmax = min(image_width, x_max)
                ymax = min(image_height, y_max)
                xmin = max(0, xmax - slice_width)
                ymin = max(0, ymax - slice_height)
                slice_bboxes.append([xmin, ymin, xmax, ymax])
            else:
                slice_bboxes.append([x_min, y_min, x_max, y_max])
            x_min = x_max - x_overlap
        y_min = y_max - y_overlap

    n_ims = 0
    sliced_image_result = []
    assert len(slice_bboxes) &gt; 0, &#34;slice image must &gt; 0&#34;
    for slice_bbox in slice_bboxes:
        n_ims += 1
        # extract image
        tlx = slice_bbox[0]
        tly = slice_bbox[1]
        brx = slice_bbox[2]
        bry = slice_bbox[3]
        image_pil_slice = image[tly:bry, tlx:brx]
        sliced_image_result.append(image_pil_slice)
    return sliced_image_result</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.Matcher.get_roi"><code class="name flex">
<span>def <span class="ident">get_roi</span></span>(<span>self, arr0, arr1)</span>
</code></dt>
<dd>
<div class="desc"><p>get overlap area
:param arr0: fov 1
:param arr1: fov 2
:return: overlap area</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_roi(self, arr0, arr1):
    &#34;&#34;&#34;
    get overlap area
    :param arr0: fov 1
    :param arr1: fov 2
    :return: overlap area
    &#34;&#34;&#34;
    h0, w0 = arr0.shape
    h1, w1 = arr1.shape
    w = int(min(w0, w1) * self.overlap)
    h = int(min(h0, h1) * self.overlap)

    if self.horizontal:
        x0, y0, x1, y1 = [w0 - w, 0, w0, h0]
        x0_, y0_, x1_, y1_ = [0, 0, w, h1]
    else:
        x0, y0, x1, y1 = [0, h0 - h, w0, h0]
        x0_, y0_, x1_, y1_ = [0, 0, w1, h]
    return arr0[y0: y1, x0: x1], arr1[y0_: y1_, x0_: x1_]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="cellbin.contrib.fov_aligner.SIFTMatcher"><code class="flex name class">
<span>class <span class="ident">SIFTMatcher</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SIFTMatcher(Matcher):
    def __init__(self, ):
        super(SIFTMatcher, self).__init__()
        self.sift = cv.SIFT_create()
        self.__matcher = self.matcher()

    def neighbor_match(self, train, query, axis=0):
        if axis == 0:
            self.horizontal = True
        else: self.horizontal = False

        train_local, query_local = self.get_roi(train, query)
        train_local = np.array(train_local, dtype=np.uint8)
        query_local = np.array(query_local, dtype=np.uint8)
        return self.sift_match(train_local, query_local)

    def sift_match(self, train, query):
        psd_kp1, psd_des1 = self.sift.detectAndCompute(train, None)
        kps1 = np.float32([kp.pt for kp in psd_kp1])
        psd_kp2, psd_des2 = self.sift.detectAndCompute(query, None)
        kps2 = np.float32([kp.pt for kp in psd_kp2])
        if len(kps1) == 0 or len(kps2) == 0:
            return None
        try:
            matches = self.__matcher.knnMatch(psd_des1, psd_des2, k=2)
        except:
            return None
        local_match = list()

        for m, n in matches:
            if m.distance &lt; 0.5 * n.distance: local_match.append((m.trainIdx, n.queryIdx))

        if len(local_match) == 0:
            return None
        else:
            pts_a = np.float32([kps1[i] for (_, i) in local_match])
            pts_b = np.float32([kps2[i] for (i, _) in local_match])
            if self.horizontal:
                pts_b[:, 0] += train.shape[1]
            else:
                pts_b[:, 1] += train.shape[0]
            offset = np.median(pts_a - pts_b, axis=0)
            return [offset[0], offset[1], 100]

    @staticmethod
    def matcher():
        flann_index_kd_tree = 1
        index_params = dict(algorithm=flann_index_kd_tree, trees=5)
        search_params = dict(checks=50)
        flann = cv.FlannBasedMatcher(index_params, search_params)
        return flann</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="cellbin.contrib.fov_aligner.Matcher" href="#cellbin.contrib.fov_aligner.Matcher">Matcher</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.SIFTMatcher.matcher"><code class="name flex">
<span>def <span class="ident">matcher</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def matcher():
    flann_index_kd_tree = 1
    index_params = dict(algorithm=flann_index_kd_tree, trees=5)
    search_params = dict(checks=50)
    flann = cv.FlannBasedMatcher(index_params, search_params)
    return flann</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cellbin.contrib.fov_aligner.SIFTMatcher.neighbor_match"><code class="name flex">
<span>def <span class="ident">neighbor_match</span></span>(<span>self, train, query, axis=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neighbor_match(self, train, query, axis=0):
    if axis == 0:
        self.horizontal = True
    else: self.horizontal = False

    train_local, query_local = self.get_roi(train, query)
    train_local = np.array(train_local, dtype=np.uint8)
    query_local = np.array(query_local, dtype=np.uint8)
    return self.sift_match(train_local, query_local)</code></pre>
</details>
</dd>
<dt id="cellbin.contrib.fov_aligner.SIFTMatcher.sift_match"><code class="name flex">
<span>def <span class="ident">sift_match</span></span>(<span>self, train, query)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sift_match(self, train, query):
    psd_kp1, psd_des1 = self.sift.detectAndCompute(train, None)
    kps1 = np.float32([kp.pt for kp in psd_kp1])
    psd_kp2, psd_des2 = self.sift.detectAndCompute(query, None)
    kps2 = np.float32([kp.pt for kp in psd_kp2])
    if len(kps1) == 0 or len(kps2) == 0:
        return None
    try:
        matches = self.__matcher.knnMatch(psd_des1, psd_des2, k=2)
    except:
        return None
    local_match = list()

    for m, n in matches:
        if m.distance &lt; 0.5 * n.distance: local_match.append((m.trainIdx, n.queryIdx))

    if len(local_match) == 0:
        return None
    else:
        pts_a = np.float32([kps1[i] for (_, i) in local_match])
        pts_b = np.float32([kps2[i] for (i, _) in local_match])
        if self.horizontal:
            pts_b[:, 0] += train.shape[1]
        else:
            pts_b[:, 1] += train.shape[0]
        offset = np.median(pts_a - pts_b, axis=0)
        return [offset[0], offset[1], 100]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="cellbin.contrib.fov_aligner.Matcher" href="#cellbin.contrib.fov_aligner.Matcher">Matcher</a></b></code>:
<ul class="hlist">
<li><code><a title="cellbin.contrib.fov_aligner.Matcher.get_roi" href="#cellbin.contrib.fov_aligner.Matcher.get_roi">get_roi</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.Matcher.slice_image" href="#cellbin.contrib.fov_aligner.Matcher.slice_image">slice_image</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cellbin.contrib" href="index.html">cellbin.contrib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cellbin.contrib.fov_aligner.FFTMatcher" href="#cellbin.contrib.fov_aligner.FFTMatcher">FFTMatcher</a></code></h4>
<ul class="">
<li><code><a title="cellbin.contrib.fov_aligner.FFTMatcher.extract_overlap_subregion" href="#cellbin.contrib.fov_aligner.FFTMatcher.extract_overlap_subregion">extract_overlap_subregion</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FFTMatcher.ncc" href="#cellbin.contrib.fov_aligner.FFTMatcher.ncc">ncc</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FFTMatcher.neighbor_match" href="#cellbin.contrib.fov_aligner.FFTMatcher.neighbor_match">neighbor_match</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FFTMatcher.pcm_peak" href="#cellbin.contrib.fov_aligner.FFTMatcher.pcm_peak">pcm_peak</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="cellbin.contrib.fov_aligner.FOVAligner" href="#cellbin.contrib.fov_aligner.FOVAligner">FOVAligner</a></code></h4>
<ul class="">
<li><code><a title="cellbin.contrib.fov_aligner.FOVAligner.create_jitter" href="#cellbin.contrib.fov_aligner.FOVAligner.create_jitter">create_jitter</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FOVAligner.filter_abnormal_offset" href="#cellbin.contrib.fov_aligner.FOVAligner.filter_abnormal_offset">filter_abnormal_offset</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FOVAligner.set_channel" href="#cellbin.contrib.fov_aligner.FOVAligner.set_channel">set_channel</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FOVAligner.set_images_path" href="#cellbin.contrib.fov_aligner.FOVAligner.set_images_path">set_images_path</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FOVAligner.set_process" href="#cellbin.contrib.fov_aligner.FOVAligner.set_process">set_process</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.FOVAligner.set_size" href="#cellbin.contrib.fov_aligner.FOVAligner.set_size">set_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="cellbin.contrib.fov_aligner.Matcher" href="#cellbin.contrib.fov_aligner.Matcher">Matcher</a></code></h4>
<ul class="">
<li><code><a title="cellbin.contrib.fov_aligner.Matcher.get_roi" href="#cellbin.contrib.fov_aligner.Matcher.get_roi">get_roi</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.Matcher.slice_image" href="#cellbin.contrib.fov_aligner.Matcher.slice_image">slice_image</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="cellbin.contrib.fov_aligner.SIFTMatcher" href="#cellbin.contrib.fov_aligner.SIFTMatcher">SIFTMatcher</a></code></h4>
<ul class="">
<li><code><a title="cellbin.contrib.fov_aligner.SIFTMatcher.matcher" href="#cellbin.contrib.fov_aligner.SIFTMatcher.matcher">matcher</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.SIFTMatcher.neighbor_match" href="#cellbin.contrib.fov_aligner.SIFTMatcher.neighbor_match">neighbor_match</a></code></li>
<li><code><a title="cellbin.contrib.fov_aligner.SIFTMatcher.sift_match" href="#cellbin.contrib.fov_aligner.SIFTMatcher.sift_match">sift_match</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>