<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>cellbin.modules.iqc.clarity_qc API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cellbin.modules.iqc.clarity_qc</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from math import ceil
from sklearn.cluster import DBSCAN

from cellbin.dnn.clarity.evaler import ClarityClassifier
from cellbin.image.augmentation import f_ij_16_to_8
from cellbin.image.augmentation import f_gray2bgr
from cellbin.utils import clog

REPRESENT_4 = {
    0: &#39;black&#39;,
    1: &#39;blur&#39;,
    2: &#39;good&#39;,
    3: &#39;over_expo&#39;,
    -1: &#39;uncertain&#39;,
}

REPRESENT_6 = {
    0: &#39;black&#39;,
    1: &#39;good&#39;,
    2: &#39;first_level_blur&#39;,
    3: &#39;second_level_blur&#39;,
    4: &#39;first_level_over_expo&#39;,
    5: &#39;second_level_over_expo&#39;,
    -1: &#39;uncertain&#39;,
}

COLOR_SET = {
    &#39;red&#39;: (255, 0, 0),
    &#39;lightsalmon&#39;: (255, 160, 122),
    &#39;blue&#39;: (0, 0, 255),
    &#39;lightblue&#39;: (173, 216, 230),
    &#39;yellow&#39;: (255, 255, 0),
    &#39;green&#39;: (0, 128, 0),
    &#39;pink&#39;: (255, 203, 192)
}

COLOR_4 = {
    &#39;black&#39;: &#39;yellow&#39;,
    &#39;blur&#39;: &#39;blue&#39;,
    &#39;good&#39;: &#39;green&#39;,
    &#39;over_expo&#39;: &#39;red&#39;,
    &#39;uncertain&#39;: &#39;pink&#39;
}

COLOR_6 = {
    &#39;black&#39;: &#39;yellow&#39;,
    &#39;first_level_blur&#39;: &#39;lightblue&#39;,
    &#39;second_level_blur&#39;: &#39;blue&#39;,
    &#39;good&#39;: &#39;green&#39;,
    &#39;first_level_over_expo&#39;: &#39;lightsalmon&#39;,
    &#39;second_level_over_expo&#39;: &#39;red&#39;,
    &#39;uncertain&#39;: &#39;pink&#39;
}

WEIGHT_MAP_6 = {
    0: 0,
    1: 1,
    2: 0.2,
    3: 0,
    4: 0.9,
    5: 0
}

WEIGHT_MAP_4 = {
    0: 0,
    1: 0,
    2: 1,
    3: 0,
}


class ClarityQC(object):
    def __init__(self, ):
        self.cl_classify = None

        self.counts = {}
        self.score = 0
        self.preds = np.array([])
        self.box_lst = []

        self.draw_img = np.array([])
        self.black_img = np.array([])
        self.fig = None
        self.topk_result = []

        self.represent = None
        self.color = None
        self.num_class = None
        self.pre_func = None
        self.weight_map = None

    def load_model(self, model_path, batch_size=2000, conf_thresh=0, gpu=&#39;-1&#39;, num_threads=0, ):
        &#34;&#34;&#34;
        Load clarity model

        Args:
            model_path (str): weight path
            batch_size (int): batch size when inferencing
            conf_thresh (): confidence threshold
            gpu (): if use gpu
            num_threads (): number of threads used in onnx session

        Returns:

        &#34;&#34;&#34;
        self.cl_classify = ClarityClassifier(
            weight_path=model_path,
            batch_size=batch_size,
            conf_thresh=conf_thresh,
            gpu=gpu,
            num_threads=num_threads,
        )
        self.pre_func = None
        self.num_class = self.cl_classify.output_shape[1]
        if self.num_class == 4:
            self.represent = REPRESENT_4
            self.color = COLOR_4
            self.weight_map = WEIGHT_MAP_4
        elif self.num_class == 6:
            self.represent = REPRESENT_6
            self.color = COLOR_6
            self.weight_map = WEIGHT_MAP_6

    def set_enhance_func(self, f):
        self.pre_func = f

    def run(self, img: np.ndarray, detect_channel=-1):
        &#34;&#34;&#34;
        This function will spilit the input image into (64, 64) pieces, then classify each piece into category.
        Category is [&#39;black&#39;, &#39;over_exposure&#39;, &#39;blur&#39;, &#39;good&#39;]

        Args:
            img (): stitched image after tissue cut (numpy ndarray)

        Returns:
            self.counts: counts of each category [&#39;black&#39;, &#39;blur&#39;, &#39;good&#39;, &#39;over_expo&#39;]
            self.score: clarity score
            self.preds: prediction in
                - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
                - 2 -&gt; 1st: class, 2nd probability
            self.boxes: the pieces coordinate
                - [[y_begin, y_end, x_begin, x_end], ...]

        &#34;&#34;&#34;
        clog.info(f&#34;Clarity eval input has {img.ndim} dims, using enhance func {self.pre_func}&#34;)
        if not isinstance(img, np.ndarray):
            raise Exception(f&#34;Only accept numpy array as input&#34;)
        if img.ndim == 3:
            if detect_channel != -1:
                img = img[:, :, detect_channel]
            elif self.pre_func is not None:
                img = self.pre_func(img, need_not=True)
        if img.dtype != np.uint8:
            img = f_ij_16_to_8(img)
        if img.ndim == 2:
            img = f_gray2bgr(img)
        self.original_img = img.copy()
        # if img.ndim != 3:
        #     img = np.expand_dims(img, axis=2)
        #     img = np.concatenate((img, img, img), axis=-1)

        counts, preds, box_lst = self.cl_classify.inference(img)
        score = self.cl_classify.score_calculator(preds, self.weight_map)
        self.counts = counts
        self.score = score
        self.preds = preds
        self.box_lst = box_lst

        # clog.info(f&#34;Clarity eval counts: {self.counts}&#34;)
        clog.info(f&#34;Clarity eval score: {self.score}&#34;)

    def post_process(self, win_h=64, win_w=64, overlap=0):
        &#34;&#34;&#34;
        This function will draw clarity classification result on image.

        Args:
            preds (): predictions from clarity result
                - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
                - 2 -&gt; 1st: class, 2nd probability
            draw_img (): the image to draw results on
            win_h (): height of image piece
            win_w (): width of image piece
            overlap (): overlap when spliting image

        Returns:
            self.draw_img: the image to draw results on

        &#34;&#34;&#34;
        import cv2
        if len(self.preds) == 0:
            return self.original_img
        h, w = self.original_img.shape[:2]
        y_nums = ceil(h / (win_h - overlap))
        x_nums = ceil(w / (win_w - overlap))
        self.black_img = np.zeros((self.original_img.shape[:2]), dtype=np.uint8)  # @jqc update
        for y_temp in range(y_nums):
            for x_temp in range(x_nums):
                x_begin = int(max(0, x_temp * (win_w - overlap)))
                y_begin = int(max(0, y_temp * (win_h - overlap)))
                x_end = int(min(x_begin + win_w, w))
                y_end = int(min(y_begin + win_h, h))
                if y_begin &gt;= y_end or x_begin &gt;= x_end:
                    continue

                cur_class, cur_score = self.preds[y_temp, x_temp]
                cur_color = COLOR_SET[self.color[self.represent[cur_class]]]
                mid_x, mid_y = int((x_begin + x_end) / 2), int((y_begin + y_end) / 2)
                cv2.putText(
                    self.original_img,
                    str(int(cur_class)) + &#34;_&#34; + str(round(cur_score * 100)),
                    (mid_x, mid_y),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.3,
                    (255, 0, 0),
                    1,
                    cv2.LINE_AA
                )

                cv2.rectangle(self.original_img, (x_begin, y_begin), (x_end, y_end), cur_color, 1)
                if cur_color == (255, 255, 0):
                    cv2.rectangle(self.black_img, (x_begin, y_begin), (x_end, y_end), cur_color, -1)
        # self.black_img[np.where(self.black_img == 0)] = 1
        # self.black_img[np.where(self.black_img == 255)] = 0
        self.black_img = (self.black_img == 0).astype(np.int8)
        draw_img = cv2.cvtColor(self.original_img, cv2.COLOR_RGB2BGR)  # if use cv2 write must do this
        self.draw_img = draw_img

    def cluster(self, top_k=10):
        &#34;&#34;&#34;
        This function is to get clustering result based on the clarity classification result.

        Args:
            preds (): predictions from clarity classification
                - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
                - 2 -&gt; 1st: class, 2nd probability
            boxes (): the pieces coordinate
                - [[y_begin, y_end, x_begin, x_end], ...]
            top_k (): top k cluster results will be returned

        Returns:
            self.fig: matplotlib figure object
                - fig.show() to show the figure
                - fig.savefig(f&#34;{path}/xx.png&#34;) to save the figure
            self.topk_result: n x 3
                - 1st element: area percent -&gt; cluster counts / tissue_counts
                - 2nd element: cluster length in x direction -&gt; x length / image width
                - 3rd element: cluster length in y direction -&gt; y length / image height

        &#34;&#34;&#34;
        import matplotlib.pyplot as plt

        preds = self.preds.reshape(-1, 2)
        preds_class = preds[:, 0: 1]
        boxes = np.array(self.box_lst)
        max_y = np.max(boxes[:, :2])
        max_x = np.max(boxes[:, 2:4])

        fig, ax = plt.subplots()
        topk_result = []

        boxes_ = []
        tissue_counts = 0
        for class_i in range(self.num_class):
            cur_class_name = self.represent[class_i]
            if self.num_class == 4 and cur_class_name not in [&#39;black&#39;, &#39;good&#39;]:
                boxes_.append(boxes[preds_class[:, 0] == class_i])
            if self.num_class == 6 and cur_class_name not in [&#39;black&#39;, &#39;good&#39;, &#39;first_level_over_expo&#39;]:
                boxes_.append(boxes[preds_class[:, 0] == class_i])
            if cur_class_name not in [&#39;black&#39;]:
                tissue_counts += (preds_class[:, 0] == class_i).sum()
        boxes_ = np.concatenate(boxes_)
        # boxes_1 = boxes[((preds_class[:, 0] == 1) | (preds_class[:, 0] == 3))]
        # tissue_counts_1 = ((preds_class[:, 0] == 1) | (preds_class[:, 0] == 3) | (preds_class[:, 0] == 2)).sum()
        y_mid = ((boxes_[:, 0] + boxes_[:, 1]) / 2).reshape(-1, 1)
        x_mid = ((boxes_[:, 2] + boxes_[:, 3]) / 2).reshape(-1, 1)
        X = np.hstack((x_mid, y_mid))  # [[x, y]]
        if len(X) != 0:
            # DBSCAN
            db = DBSCAN(eps=91, min_samples=4).fit(X)
            labels = db.labels_
            # Number of clusters in labels, ignoring noise if present.
            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)

            if n_clusters_ != 0:
                n_noise_ = list(labels).count(-1)

                # print(&#34;Estimated number of clusters: %d&#34; % n_clusters_)
                # print(&#34;Estimated number of noise points: %d&#34; % n_noise_)

                core_samples_mask = np.zeros_like(labels, dtype=bool)
                core_samples_mask[db.core_sample_indices_] = True

                unique_ls, counts = np.unique(labels[core_samples_mask], return_counts=True)
                unique_ls_sort = unique_ls[counts.argsort()[::-1]]

                topk = top_k
                topk = min(topk, n_clusters_)
                unique_ls_sort = unique_ls_sort[: topk]
                colors = [plt.cm.tab20(each) for each in np.linspace(0, 1, topk)]

                index = 1

                # draw
                for k, col in zip(unique_ls_sort, colors):
                    if k == -1:
                        # Black used for noise.
                        # col = [0, 0, 0, 1]
                        continue

                    class_member_mask = labels == k
                    xy = X[class_member_mask &amp; core_samples_mask]

                    cluster_x = xy[:, 0]
                    cluster_minx, cluster_maxx = np.min(cluster_x), np.max(cluster_x)
                    cluster_y = xy[:, 1]
                    cluster_miny, cluster_maxy = np.min(cluster_y), np.max(cluster_y)
                    x_len = cluster_maxx - cluster_minx
                    y_len = cluster_maxy - cluster_miny
                    x_len_pt = x_len / max_x
                    y_len_pt = y_len / max_y
                    topk_result.append([len(xy) / tissue_counts, x_len_pt, y_len_pt])

                    # topk_result[index] = len(xy)
                    ax.plot(
                        xy[:, 0],
                        xy[:, 1],
                        &#34;o&#34;,
                        markerfacecolor=tuple(col),
                        markeredgecolor=&#34;k&#34;,
                        markersize=5,
                    )
                    index += 1
                    # xy = X[class_member_mask &amp; ~core_samples_mask]
                    # plt.plot(
                    #     xy[:, 0],
                    #     xy[:, 1],
                    #     &#34;o&#34;,
                    #     markerfacecolor=tuple(col),
                    #     markeredgecolor=&#34;k&#34;,
                    #     markersize=5,
                    # )
                    # break
                scale = (max_x // 100) * 3
                plt.xlim([-scale, max_x + scale])
                plt.ylim([-scale, max_y + scale])
                plt.gca().invert_yaxis()
                plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;)

                topk_result = np.round(topk_result, 3)
                # plt.title(f&#34;Estimated number of clusters: {n_clusters_}&#34;)
                largest = topk_result[0]
                plt.title(
                    f&#34;Top {topk} clusters \n Largest cluster area: {largest[0]}, width: {largest[1]}, height: {largest[2]}&#34;)
            plt.tight_layout()
        self.fig = fig
        self.topk_result = topk_result


if __name__ == &#39;__main__&#39;:
    import cv2

    weight_path = r&#34;D:\PycharmProjects\imageqc-beta\ImageQC\models\ST_TP_Mobile_small_050_V2.onnx&#34;
    batch_size = 2000
    clarity_qc = ClarityQC()
    clarity_qc.load_model(weight_path)
    print(&#34;asd&#34;)

    img_path = r&#34;D:\Data\qc\new_qc_test_data\clarity\bad\test_imgs\test_output\fovs_test\fov_stitched_transform.tif&#34;
    img = cv2.imread(img_path, -1)
    clarity_qc.run(img)
    clarity_qc.post_process()
    # cv2.imwrite(r&#34;D:\Data\qc\SS200000302TL_B5_out\new_qc_out\test1.tif&#34;, clarity_qc.draw_img)
    # clarity_qc.cluster()
    # clarity_qc.fig.savefig(r&#34;D:\Data\qc\SS200000302TL_B5_out\new_qc_out\test1.png&#34;,)
    # tissue_sum = counts[1] + counts[2] + counts[3]
    # topk_pt = (topk_result / tissue_sum) * 100  # topk cluster area percentage
    # print(&#34;asd&#34;)
    from cellbin.dnn.tseg.yolo.detector import TissueSegmentationYolo
    import tifffile

    seg = TissueSegmentationYolo()
    seg.f_init_model(r&#34;D:\PycharmProjects\cellbin\cellbin\dnn\weights\tissueseg_yolo_SH_20230131_th.onnx&#34;, )
    img = tifffile.imread(img_path)
    mask = seg.f_predict(img)
    iou_result = iou(clarity_qc.black_img, mask)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cellbin.modules.iqc.clarity_qc.ClarityQC"><code class="flex name class">
<span>class <span class="ident">ClarityQC</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClarityQC(object):
    def __init__(self, ):
        self.cl_classify = None

        self.counts = {}
        self.score = 0
        self.preds = np.array([])
        self.box_lst = []

        self.draw_img = np.array([])
        self.black_img = np.array([])
        self.fig = None
        self.topk_result = []

        self.represent = None
        self.color = None
        self.num_class = None
        self.pre_func = None
        self.weight_map = None

    def load_model(self, model_path, batch_size=2000, conf_thresh=0, gpu=&#39;-1&#39;, num_threads=0, ):
        &#34;&#34;&#34;
        Load clarity model

        Args:
            model_path (str): weight path
            batch_size (int): batch size when inferencing
            conf_thresh (): confidence threshold
            gpu (): if use gpu
            num_threads (): number of threads used in onnx session

        Returns:

        &#34;&#34;&#34;
        self.cl_classify = ClarityClassifier(
            weight_path=model_path,
            batch_size=batch_size,
            conf_thresh=conf_thresh,
            gpu=gpu,
            num_threads=num_threads,
        )
        self.pre_func = None
        self.num_class = self.cl_classify.output_shape[1]
        if self.num_class == 4:
            self.represent = REPRESENT_4
            self.color = COLOR_4
            self.weight_map = WEIGHT_MAP_4
        elif self.num_class == 6:
            self.represent = REPRESENT_6
            self.color = COLOR_6
            self.weight_map = WEIGHT_MAP_6

    def set_enhance_func(self, f):
        self.pre_func = f

    def run(self, img: np.ndarray, detect_channel=-1):
        &#34;&#34;&#34;
        This function will spilit the input image into (64, 64) pieces, then classify each piece into category.
        Category is [&#39;black&#39;, &#39;over_exposure&#39;, &#39;blur&#39;, &#39;good&#39;]

        Args:
            img (): stitched image after tissue cut (numpy ndarray)

        Returns:
            self.counts: counts of each category [&#39;black&#39;, &#39;blur&#39;, &#39;good&#39;, &#39;over_expo&#39;]
            self.score: clarity score
            self.preds: prediction in
                - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
                - 2 -&gt; 1st: class, 2nd probability
            self.boxes: the pieces coordinate
                - [[y_begin, y_end, x_begin, x_end], ...]

        &#34;&#34;&#34;
        clog.info(f&#34;Clarity eval input has {img.ndim} dims, using enhance func {self.pre_func}&#34;)
        if not isinstance(img, np.ndarray):
            raise Exception(f&#34;Only accept numpy array as input&#34;)
        if img.ndim == 3:
            if detect_channel != -1:
                img = img[:, :, detect_channel]
            elif self.pre_func is not None:
                img = self.pre_func(img, need_not=True)
        if img.dtype != np.uint8:
            img = f_ij_16_to_8(img)
        if img.ndim == 2:
            img = f_gray2bgr(img)
        self.original_img = img.copy()
        # if img.ndim != 3:
        #     img = np.expand_dims(img, axis=2)
        #     img = np.concatenate((img, img, img), axis=-1)

        counts, preds, box_lst = self.cl_classify.inference(img)
        score = self.cl_classify.score_calculator(preds, self.weight_map)
        self.counts = counts
        self.score = score
        self.preds = preds
        self.box_lst = box_lst

        # clog.info(f&#34;Clarity eval counts: {self.counts}&#34;)
        clog.info(f&#34;Clarity eval score: {self.score}&#34;)

    def post_process(self, win_h=64, win_w=64, overlap=0):
        &#34;&#34;&#34;
        This function will draw clarity classification result on image.

        Args:
            preds (): predictions from clarity result
                - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
                - 2 -&gt; 1st: class, 2nd probability
            draw_img (): the image to draw results on
            win_h (): height of image piece
            win_w (): width of image piece
            overlap (): overlap when spliting image

        Returns:
            self.draw_img: the image to draw results on

        &#34;&#34;&#34;
        import cv2
        if len(self.preds) == 0:
            return self.original_img
        h, w = self.original_img.shape[:2]
        y_nums = ceil(h / (win_h - overlap))
        x_nums = ceil(w / (win_w - overlap))
        self.black_img = np.zeros((self.original_img.shape[:2]), dtype=np.uint8)  # @jqc update
        for y_temp in range(y_nums):
            for x_temp in range(x_nums):
                x_begin = int(max(0, x_temp * (win_w - overlap)))
                y_begin = int(max(0, y_temp * (win_h - overlap)))
                x_end = int(min(x_begin + win_w, w))
                y_end = int(min(y_begin + win_h, h))
                if y_begin &gt;= y_end or x_begin &gt;= x_end:
                    continue

                cur_class, cur_score = self.preds[y_temp, x_temp]
                cur_color = COLOR_SET[self.color[self.represent[cur_class]]]
                mid_x, mid_y = int((x_begin + x_end) / 2), int((y_begin + y_end) / 2)
                cv2.putText(
                    self.original_img,
                    str(int(cur_class)) + &#34;_&#34; + str(round(cur_score * 100)),
                    (mid_x, mid_y),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.3,
                    (255, 0, 0),
                    1,
                    cv2.LINE_AA
                )

                cv2.rectangle(self.original_img, (x_begin, y_begin), (x_end, y_end), cur_color, 1)
                if cur_color == (255, 255, 0):
                    cv2.rectangle(self.black_img, (x_begin, y_begin), (x_end, y_end), cur_color, -1)
        # self.black_img[np.where(self.black_img == 0)] = 1
        # self.black_img[np.where(self.black_img == 255)] = 0
        self.black_img = (self.black_img == 0).astype(np.int8)
        draw_img = cv2.cvtColor(self.original_img, cv2.COLOR_RGB2BGR)  # if use cv2 write must do this
        self.draw_img = draw_img

    def cluster(self, top_k=10):
        &#34;&#34;&#34;
        This function is to get clustering result based on the clarity classification result.

        Args:
            preds (): predictions from clarity classification
                - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
                - 2 -&gt; 1st: class, 2nd probability
            boxes (): the pieces coordinate
                - [[y_begin, y_end, x_begin, x_end], ...]
            top_k (): top k cluster results will be returned

        Returns:
            self.fig: matplotlib figure object
                - fig.show() to show the figure
                - fig.savefig(f&#34;{path}/xx.png&#34;) to save the figure
            self.topk_result: n x 3
                - 1st element: area percent -&gt; cluster counts / tissue_counts
                - 2nd element: cluster length in x direction -&gt; x length / image width
                - 3rd element: cluster length in y direction -&gt; y length / image height

        &#34;&#34;&#34;
        import matplotlib.pyplot as plt

        preds = self.preds.reshape(-1, 2)
        preds_class = preds[:, 0: 1]
        boxes = np.array(self.box_lst)
        max_y = np.max(boxes[:, :2])
        max_x = np.max(boxes[:, 2:4])

        fig, ax = plt.subplots()
        topk_result = []

        boxes_ = []
        tissue_counts = 0
        for class_i in range(self.num_class):
            cur_class_name = self.represent[class_i]
            if self.num_class == 4 and cur_class_name not in [&#39;black&#39;, &#39;good&#39;]:
                boxes_.append(boxes[preds_class[:, 0] == class_i])
            if self.num_class == 6 and cur_class_name not in [&#39;black&#39;, &#39;good&#39;, &#39;first_level_over_expo&#39;]:
                boxes_.append(boxes[preds_class[:, 0] == class_i])
            if cur_class_name not in [&#39;black&#39;]:
                tissue_counts += (preds_class[:, 0] == class_i).sum()
        boxes_ = np.concatenate(boxes_)
        # boxes_1 = boxes[((preds_class[:, 0] == 1) | (preds_class[:, 0] == 3))]
        # tissue_counts_1 = ((preds_class[:, 0] == 1) | (preds_class[:, 0] == 3) | (preds_class[:, 0] == 2)).sum()
        y_mid = ((boxes_[:, 0] + boxes_[:, 1]) / 2).reshape(-1, 1)
        x_mid = ((boxes_[:, 2] + boxes_[:, 3]) / 2).reshape(-1, 1)
        X = np.hstack((x_mid, y_mid))  # [[x, y]]
        if len(X) != 0:
            # DBSCAN
            db = DBSCAN(eps=91, min_samples=4).fit(X)
            labels = db.labels_
            # Number of clusters in labels, ignoring noise if present.
            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)

            if n_clusters_ != 0:
                n_noise_ = list(labels).count(-1)

                # print(&#34;Estimated number of clusters: %d&#34; % n_clusters_)
                # print(&#34;Estimated number of noise points: %d&#34; % n_noise_)

                core_samples_mask = np.zeros_like(labels, dtype=bool)
                core_samples_mask[db.core_sample_indices_] = True

                unique_ls, counts = np.unique(labels[core_samples_mask], return_counts=True)
                unique_ls_sort = unique_ls[counts.argsort()[::-1]]

                topk = top_k
                topk = min(topk, n_clusters_)
                unique_ls_sort = unique_ls_sort[: topk]
                colors = [plt.cm.tab20(each) for each in np.linspace(0, 1, topk)]

                index = 1

                # draw
                for k, col in zip(unique_ls_sort, colors):
                    if k == -1:
                        # Black used for noise.
                        # col = [0, 0, 0, 1]
                        continue

                    class_member_mask = labels == k
                    xy = X[class_member_mask &amp; core_samples_mask]

                    cluster_x = xy[:, 0]
                    cluster_minx, cluster_maxx = np.min(cluster_x), np.max(cluster_x)
                    cluster_y = xy[:, 1]
                    cluster_miny, cluster_maxy = np.min(cluster_y), np.max(cluster_y)
                    x_len = cluster_maxx - cluster_minx
                    y_len = cluster_maxy - cluster_miny
                    x_len_pt = x_len / max_x
                    y_len_pt = y_len / max_y
                    topk_result.append([len(xy) / tissue_counts, x_len_pt, y_len_pt])

                    # topk_result[index] = len(xy)
                    ax.plot(
                        xy[:, 0],
                        xy[:, 1],
                        &#34;o&#34;,
                        markerfacecolor=tuple(col),
                        markeredgecolor=&#34;k&#34;,
                        markersize=5,
                    )
                    index += 1
                    # xy = X[class_member_mask &amp; ~core_samples_mask]
                    # plt.plot(
                    #     xy[:, 0],
                    #     xy[:, 1],
                    #     &#34;o&#34;,
                    #     markerfacecolor=tuple(col),
                    #     markeredgecolor=&#34;k&#34;,
                    #     markersize=5,
                    # )
                    # break
                scale = (max_x // 100) * 3
                plt.xlim([-scale, max_x + scale])
                plt.ylim([-scale, max_y + scale])
                plt.gca().invert_yaxis()
                plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;)

                topk_result = np.round(topk_result, 3)
                # plt.title(f&#34;Estimated number of clusters: {n_clusters_}&#34;)
                largest = topk_result[0]
                plt.title(
                    f&#34;Top {topk} clusters \n Largest cluster area: {largest[0]}, width: {largest[1]}, height: {largest[2]}&#34;)
            plt.tight_layout()
        self.fig = fig
        self.topk_result = topk_result</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="cellbin.modules.iqc.clarity_qc.ClarityQC.cluster"><code class="name flex">
<span>def <span class="ident">cluster</span></span>(<span>self, top_k=10)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is to get clustering result based on the clarity classification result.</p>
<h2 id="args">Args</h2>
<p>preds (): predictions from clarity classification
- shape is ceil(image_height / (64 - _overlap)),
ceil(image_width / (64 - self._overlap), 2)
- 2 -&gt; 1st: class, 2nd probability
boxes (): the pieces coordinate
- [[y_begin, y_end, x_begin, x_end], &hellip;]
top_k (): top k cluster results will be returned</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self.fig</code></dt>
<dd>matplotlib figure object
- fig.show() to show the figure
- fig.savefig(f"{path}/xx.png") to save the figure</dd>
<dt><code>self.topk_result</code></dt>
<dd>n x 3
- 1st element: area percent -&gt; cluster counts / tissue_counts
- 2nd element: cluster length in x direction -&gt; x length / image width
- 3rd element: cluster length in y direction -&gt; y length / image height</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster(self, top_k=10):
    &#34;&#34;&#34;
    This function is to get clustering result based on the clarity classification result.

    Args:
        preds (): predictions from clarity classification
            - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
            - 2 -&gt; 1st: class, 2nd probability
        boxes (): the pieces coordinate
            - [[y_begin, y_end, x_begin, x_end], ...]
        top_k (): top k cluster results will be returned

    Returns:
        self.fig: matplotlib figure object
            - fig.show() to show the figure
            - fig.savefig(f&#34;{path}/xx.png&#34;) to save the figure
        self.topk_result: n x 3
            - 1st element: area percent -&gt; cluster counts / tissue_counts
            - 2nd element: cluster length in x direction -&gt; x length / image width
            - 3rd element: cluster length in y direction -&gt; y length / image height

    &#34;&#34;&#34;
    import matplotlib.pyplot as plt

    preds = self.preds.reshape(-1, 2)
    preds_class = preds[:, 0: 1]
    boxes = np.array(self.box_lst)
    max_y = np.max(boxes[:, :2])
    max_x = np.max(boxes[:, 2:4])

    fig, ax = plt.subplots()
    topk_result = []

    boxes_ = []
    tissue_counts = 0
    for class_i in range(self.num_class):
        cur_class_name = self.represent[class_i]
        if self.num_class == 4 and cur_class_name not in [&#39;black&#39;, &#39;good&#39;]:
            boxes_.append(boxes[preds_class[:, 0] == class_i])
        if self.num_class == 6 and cur_class_name not in [&#39;black&#39;, &#39;good&#39;, &#39;first_level_over_expo&#39;]:
            boxes_.append(boxes[preds_class[:, 0] == class_i])
        if cur_class_name not in [&#39;black&#39;]:
            tissue_counts += (preds_class[:, 0] == class_i).sum()
    boxes_ = np.concatenate(boxes_)
    # boxes_1 = boxes[((preds_class[:, 0] == 1) | (preds_class[:, 0] == 3))]
    # tissue_counts_1 = ((preds_class[:, 0] == 1) | (preds_class[:, 0] == 3) | (preds_class[:, 0] == 2)).sum()
    y_mid = ((boxes_[:, 0] + boxes_[:, 1]) / 2).reshape(-1, 1)
    x_mid = ((boxes_[:, 2] + boxes_[:, 3]) / 2).reshape(-1, 1)
    X = np.hstack((x_mid, y_mid))  # [[x, y]]
    if len(X) != 0:
        # DBSCAN
        db = DBSCAN(eps=91, min_samples=4).fit(X)
        labels = db.labels_
        # Number of clusters in labels, ignoring noise if present.
        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)

        if n_clusters_ != 0:
            n_noise_ = list(labels).count(-1)

            # print(&#34;Estimated number of clusters: %d&#34; % n_clusters_)
            # print(&#34;Estimated number of noise points: %d&#34; % n_noise_)

            core_samples_mask = np.zeros_like(labels, dtype=bool)
            core_samples_mask[db.core_sample_indices_] = True

            unique_ls, counts = np.unique(labels[core_samples_mask], return_counts=True)
            unique_ls_sort = unique_ls[counts.argsort()[::-1]]

            topk = top_k
            topk = min(topk, n_clusters_)
            unique_ls_sort = unique_ls_sort[: topk]
            colors = [plt.cm.tab20(each) for each in np.linspace(0, 1, topk)]

            index = 1

            # draw
            for k, col in zip(unique_ls_sort, colors):
                if k == -1:
                    # Black used for noise.
                    # col = [0, 0, 0, 1]
                    continue

                class_member_mask = labels == k
                xy = X[class_member_mask &amp; core_samples_mask]

                cluster_x = xy[:, 0]
                cluster_minx, cluster_maxx = np.min(cluster_x), np.max(cluster_x)
                cluster_y = xy[:, 1]
                cluster_miny, cluster_maxy = np.min(cluster_y), np.max(cluster_y)
                x_len = cluster_maxx - cluster_minx
                y_len = cluster_maxy - cluster_miny
                x_len_pt = x_len / max_x
                y_len_pt = y_len / max_y
                topk_result.append([len(xy) / tissue_counts, x_len_pt, y_len_pt])

                # topk_result[index] = len(xy)
                ax.plot(
                    xy[:, 0],
                    xy[:, 1],
                    &#34;o&#34;,
                    markerfacecolor=tuple(col),
                    markeredgecolor=&#34;k&#34;,
                    markersize=5,
                )
                index += 1
                # xy = X[class_member_mask &amp; ~core_samples_mask]
                # plt.plot(
                #     xy[:, 0],
                #     xy[:, 1],
                #     &#34;o&#34;,
                #     markerfacecolor=tuple(col),
                #     markeredgecolor=&#34;k&#34;,
                #     markersize=5,
                # )
                # break
            scale = (max_x // 100) * 3
            plt.xlim([-scale, max_x + scale])
            plt.ylim([-scale, max_y + scale])
            plt.gca().invert_yaxis()
            plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;)

            topk_result = np.round(topk_result, 3)
            # plt.title(f&#34;Estimated number of clusters: {n_clusters_}&#34;)
            largest = topk_result[0]
            plt.title(
                f&#34;Top {topk} clusters \n Largest cluster area: {largest[0]}, width: {largest[1]}, height: {largest[2]}&#34;)
        plt.tight_layout()
    self.fig = fig
    self.topk_result = topk_result</code></pre>
</details>
</dd>
<dt id="cellbin.modules.iqc.clarity_qc.ClarityQC.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self, model_path, batch_size=2000, conf_thresh=0, gpu='-1', num_threads=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Load clarity model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>weight path</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>batch size when inferencing</dd>
</dl>
<p>conf_thresh (): confidence threshold
gpu (): if use gpu
num_threads (): number of threads used in onnx session
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self, model_path, batch_size=2000, conf_thresh=0, gpu=&#39;-1&#39;, num_threads=0, ):
    &#34;&#34;&#34;
    Load clarity model

    Args:
        model_path (str): weight path
        batch_size (int): batch size when inferencing
        conf_thresh (): confidence threshold
        gpu (): if use gpu
        num_threads (): number of threads used in onnx session

    Returns:

    &#34;&#34;&#34;
    self.cl_classify = ClarityClassifier(
        weight_path=model_path,
        batch_size=batch_size,
        conf_thresh=conf_thresh,
        gpu=gpu,
        num_threads=num_threads,
    )
    self.pre_func = None
    self.num_class = self.cl_classify.output_shape[1]
    if self.num_class == 4:
        self.represent = REPRESENT_4
        self.color = COLOR_4
        self.weight_map = WEIGHT_MAP_4
    elif self.num_class == 6:
        self.represent = REPRESENT_6
        self.color = COLOR_6
        self.weight_map = WEIGHT_MAP_6</code></pre>
</details>
</dd>
<dt id="cellbin.modules.iqc.clarity_qc.ClarityQC.post_process"><code class="name flex">
<span>def <span class="ident">post_process</span></span>(<span>self, win_h=64, win_w=64, overlap=0)</span>
</code></dt>
<dd>
<div class="desc"><p>This function will draw clarity classification result on image.</p>
<h2 id="args">Args</h2>
<p>preds (): predictions from clarity result
- shape is ceil(image_height / (64 - _overlap)),
ceil(image_width / (64 - self._overlap), 2)
- 2 -&gt; 1st: class, 2nd probability
draw_img (): the image to draw results on
win_h (): height of image piece
win_w (): width of image piece
overlap (): overlap when spliting image</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self.draw_img</code></dt>
<dd>the image to draw results on</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_process(self, win_h=64, win_w=64, overlap=0):
    &#34;&#34;&#34;
    This function will draw clarity classification result on image.

    Args:
        preds (): predictions from clarity result
            - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
            - 2 -&gt; 1st: class, 2nd probability
        draw_img (): the image to draw results on
        win_h (): height of image piece
        win_w (): width of image piece
        overlap (): overlap when spliting image

    Returns:
        self.draw_img: the image to draw results on

    &#34;&#34;&#34;
    import cv2
    if len(self.preds) == 0:
        return self.original_img
    h, w = self.original_img.shape[:2]
    y_nums = ceil(h / (win_h - overlap))
    x_nums = ceil(w / (win_w - overlap))
    self.black_img = np.zeros((self.original_img.shape[:2]), dtype=np.uint8)  # @jqc update
    for y_temp in range(y_nums):
        for x_temp in range(x_nums):
            x_begin = int(max(0, x_temp * (win_w - overlap)))
            y_begin = int(max(0, y_temp * (win_h - overlap)))
            x_end = int(min(x_begin + win_w, w))
            y_end = int(min(y_begin + win_h, h))
            if y_begin &gt;= y_end or x_begin &gt;= x_end:
                continue

            cur_class, cur_score = self.preds[y_temp, x_temp]
            cur_color = COLOR_SET[self.color[self.represent[cur_class]]]
            mid_x, mid_y = int((x_begin + x_end) / 2), int((y_begin + y_end) / 2)
            cv2.putText(
                self.original_img,
                str(int(cur_class)) + &#34;_&#34; + str(round(cur_score * 100)),
                (mid_x, mid_y),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.3,
                (255, 0, 0),
                1,
                cv2.LINE_AA
            )

            cv2.rectangle(self.original_img, (x_begin, y_begin), (x_end, y_end), cur_color, 1)
            if cur_color == (255, 255, 0):
                cv2.rectangle(self.black_img, (x_begin, y_begin), (x_end, y_end), cur_color, -1)
    # self.black_img[np.where(self.black_img == 0)] = 1
    # self.black_img[np.where(self.black_img == 255)] = 0
    self.black_img = (self.black_img == 0).astype(np.int8)
    draw_img = cv2.cvtColor(self.original_img, cv2.COLOR_RGB2BGR)  # if use cv2 write must do this
    self.draw_img = draw_img</code></pre>
</details>
</dd>
<dt id="cellbin.modules.iqc.clarity_qc.ClarityQC.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, img: numpy.ndarray, detect_channel=-1)</span>
</code></dt>
<dd>
<div class="desc"><p>This function will spilit the input image into (64, 64) pieces, then classify each piece into category.
Category is ['black', 'over_exposure', 'blur', 'good']</p>
<h2 id="args">Args</h2>
<p>img (): stitched image after tissue cut (numpy ndarray)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self.counts</code></dt>
<dd>counts of each category ['black', 'blur', 'good', 'over_expo']</dd>
<dt><code>self.score</code></dt>
<dd>clarity score</dd>
<dt><code>self.preds</code></dt>
<dd>prediction in
- shape is ceil(image_height / (64 - _overlap)),
ceil(image_width / (64 - self._overlap), 2)
- 2 -&gt; 1st: class, 2nd probability</dd>
<dt><code>self.boxes</code></dt>
<dd>the pieces coordinate
- [[y_begin, y_end, x_begin, x_end], &hellip;]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, img: np.ndarray, detect_channel=-1):
    &#34;&#34;&#34;
    This function will spilit the input image into (64, 64) pieces, then classify each piece into category.
    Category is [&#39;black&#39;, &#39;over_exposure&#39;, &#39;blur&#39;, &#39;good&#39;]

    Args:
        img (): stitched image after tissue cut (numpy ndarray)

    Returns:
        self.counts: counts of each category [&#39;black&#39;, &#39;blur&#39;, &#39;good&#39;, &#39;over_expo&#39;]
        self.score: clarity score
        self.preds: prediction in
            - shape is ceil(image_height / (64 - _overlap)),  ceil(image_width / (64 - self._overlap), 2)
            - 2 -&gt; 1st: class, 2nd probability
        self.boxes: the pieces coordinate
            - [[y_begin, y_end, x_begin, x_end], ...]

    &#34;&#34;&#34;
    clog.info(f&#34;Clarity eval input has {img.ndim} dims, using enhance func {self.pre_func}&#34;)
    if not isinstance(img, np.ndarray):
        raise Exception(f&#34;Only accept numpy array as input&#34;)
    if img.ndim == 3:
        if detect_channel != -1:
            img = img[:, :, detect_channel]
        elif self.pre_func is not None:
            img = self.pre_func(img, need_not=True)
    if img.dtype != np.uint8:
        img = f_ij_16_to_8(img)
    if img.ndim == 2:
        img = f_gray2bgr(img)
    self.original_img = img.copy()
    # if img.ndim != 3:
    #     img = np.expand_dims(img, axis=2)
    #     img = np.concatenate((img, img, img), axis=-1)

    counts, preds, box_lst = self.cl_classify.inference(img)
    score = self.cl_classify.score_calculator(preds, self.weight_map)
    self.counts = counts
    self.score = score
    self.preds = preds
    self.box_lst = box_lst

    # clog.info(f&#34;Clarity eval counts: {self.counts}&#34;)
    clog.info(f&#34;Clarity eval score: {self.score}&#34;)</code></pre>
</details>
</dd>
<dt id="cellbin.modules.iqc.clarity_qc.ClarityQC.set_enhance_func"><code class="name flex">
<span>def <span class="ident">set_enhance_func</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_enhance_func(self, f):
    self.pre_func = f</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cellbin.modules.iqc" href="index.html">cellbin.modules.iqc</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cellbin.modules.iqc.clarity_qc.ClarityQC" href="#cellbin.modules.iqc.clarity_qc.ClarityQC">ClarityQC</a></code></h4>
<ul class="">
<li><code><a title="cellbin.modules.iqc.clarity_qc.ClarityQC.cluster" href="#cellbin.modules.iqc.clarity_qc.ClarityQC.cluster">cluster</a></code></li>
<li><code><a title="cellbin.modules.iqc.clarity_qc.ClarityQC.load_model" href="#cellbin.modules.iqc.clarity_qc.ClarityQC.load_model">load_model</a></code></li>
<li><code><a title="cellbin.modules.iqc.clarity_qc.ClarityQC.post_process" href="#cellbin.modules.iqc.clarity_qc.ClarityQC.post_process">post_process</a></code></li>
<li><code><a title="cellbin.modules.iqc.clarity_qc.ClarityQC.run" href="#cellbin.modules.iqc.clarity_qc.ClarityQC.run">run</a></code></li>
<li><code><a title="cellbin.modules.iqc.clarity_qc.ClarityQC.set_enhance_func" href="#cellbin.modules.iqc.clarity_qc.ClarityQC.set_enhance_func">set_enhance_func</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>